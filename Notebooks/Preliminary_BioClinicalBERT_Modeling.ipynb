{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829d14e1-096e-4076-898d-0135c5b35401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:02:52.493864Z",
     "iopub.status.busy": "2024-09-10T05:02:52.492001Z",
     "iopub.status.idle": "2024-09-10T05:03:21.534814Z",
     "shell.execute_reply": "2024-09-10T05:03:21.534183Z"
    }
   },
   "source": [
    "# Preliminary Modelling - Pretrained Bio_ClinicalBERT \n",
    "#### Hugging face: https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
    "\n",
    "#### During preliminary modeling, only the gradients of the classification head parameters are optimized during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05674675-a8eb-4b92-a0a7-012112dc54c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:02:52.493864Z",
     "iopub.status.busy": "2024-09-10T05:02:52.492001Z",
     "iopub.status.idle": "2024-09-10T05:03:21.534814Z",
     "shell.execute_reply": "2024-09-10T05:03:21.534183Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bigdata/Software/python3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75eed9f3-b269-4cb7-bf3c-03569d40ba98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:03:21.541184Z",
     "iopub.status.busy": "2024-09-10T05:03:21.539595Z",
     "iopub.status.idle": "2024-09-10T05:03:31.736055Z",
     "shell.execute_reply": "2024-09-10T05:03:31.735508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model from hugging face\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", clean_up_tokenization_spaces = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ea7751-f1fd-45f8-aeaa-4c06827ff53c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:03:31.741703Z",
     "iopub.status.busy": "2024-09-10T05:03:31.740221Z",
     "iopub.status.idle": "2024-09-10T05:03:37.172020Z",
     "shell.execute_reply": "2024-09-10T05:03:37.171514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8715131-cae0-4c28-bc30-3f2e230d4207",
   "metadata": {},
   "source": [
    "#### Freezing the original weights of the pretrained model trained on the MIMIC text corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfa86f8-3334-427c-be44-5b59989f6bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:03:37.177165Z",
     "iopub.status.busy": "2024-09-10T05:03:37.175799Z",
     "iopub.status.idle": "2024-09-10T05:03:37.188863Z",
     "shell.execute_reply": "2024-09-10T05:03:37.188337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.requires_grad_(False) #Freeze the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52cf3be-04b2-44e7-9e4a-b59fb8c1d108",
   "metadata": {},
   "source": [
    "Pre-trained weights are loaded for the layers inside the `BertModel` (embeddings, encoder, pooler). These weights have been trained on clinical text data when the model is loaded from Hugging Face, such as Bio_ClinicalBERT.\n",
    "\n",
    "The classifier, which includes `classifier.bias` and `classifier.weight`, is not pre-trained. It is newly initialized since the model does not yet know the specific task being performed (e.g., binary classification). This layer requires training (fine-tuning) on the specific task to achieve optimal performance.\n",
    "\n",
    "The weights of the pre-trained model's parameters can be frozen to prevent them from being updated during backpropagation while fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e13952bc-6d85-4d47-92c6-b91171c52d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:03:37.194324Z",
     "iopub.status.busy": "2024-09-10T05:03:37.192883Z",
     "iopub.status.idle": "2024-09-10T05:03:38.087843Z",
     "shell.execute_reply": "2024-09-10T05:03:38.087260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45359 entries, 0 to 45571\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   HADM_ID  45359 non-null  float64\n",
      " 1   text     45359 non-null  object \n",
      " 2   label    45359 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.4+ MB\n",
      "Dataset info: \n",
      " None\n",
      "Dataset first 5 rows: \n",
      "     HADM_ID                                               text  label\n",
      "0  134640.0  Pedestrian struck by motor vehicle, 60 year ol...      1\n",
      "1  127159.0  involved in a motor vehicle accident. She was ...      1\n",
      "2  188655.0  post motor vehicle accident on **2132-7-9**, s...      1\n",
      "3  188655.0  male who was a restrained passenger in a 110 m...      1\n",
      "4  191263.0  This is a 47 year old female who was in a moto...      1\n"
     ]
    }
   ],
   "source": [
    "# Load the train dataset\n",
    "train = pd.read_csv(\"train.csv\").dropna()\n",
    "\n",
    "# Ensure that labels are integers\n",
    "train['label'] = train['label'].astype(int)\n",
    "\n",
    "# Sample 1000 rows from the dataset\n",
    "# train = train.sample(1000)\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Dataset info: \\n {train.info()}\")\n",
    "print(f\"Dataset first 5 rows: \\n {train.head(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b51cd44-1e03-43c5-a07b-a9ee9c1f5e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:03:38.094298Z",
     "iopub.status.busy": "2024-09-10T05:03:38.092601Z",
     "iopub.status.idle": "2024-09-10T05:04:15.026803Z",
     "shell.execute_reply": "2024-09-10T05:04:15.026255Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract texts and labels from the dataset\n",
    "texts = train['text'].tolist()  # Convert the 'text' column to a list\n",
    "labels = train['label'].tolist()  # Convert the 'label' column to a list\n",
    "\n",
    "# Tokenize the dataset with padding, truncation, and conversion to tensors\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(labels)  # Convert list of labels to a tensor\n",
    "\n",
    "# Create TensorDataset using tokenized inputs and labels\n",
    "data = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "\n",
    "# Create DataLoader for the entire dataset\n",
    "dataLoader = DataLoader(data, batch_size=16)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd27de8-ad4e-49d0-8bd8-5bd50d0325f6",
   "metadata": {},
   "source": [
    "#### Testing with randomly initialised weights of the classifier head (without finetuning), test on labelled raw text data extracted from MIMIC-III\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d9df15-1dc0-4a52-bdf2-4fb9d9576c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:04:15.032958Z",
     "iopub.status.busy": "2024-09-10T05:04:15.031308Z",
     "iopub.status.idle": "2024-09-10T05:09:58.127936Z",
     "shell.execute_reply": "2024-09-10T05:09:58.127339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5546\n",
      "Misclassification Error Rate: 0.4454\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.66      0.22      0.33     22573\n",
      "     Class 1       0.53      0.89      0.67     22786\n",
      "\n",
      "    accuracy                           0.55     45359\n",
      "   macro avg       0.60      0.55      0.50     45359\n",
      "weighted avg       0.59      0.55      0.50     45359\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to predict labels using the model without fine-tuning\n",
    "def predictLabels(dataLoader):\n",
    "    predictions = []\n",
    "    for batch in dataLoader:\n",
    "        bInputIds, bInputMask, _ = [t.to(device) for t in batch]  # Move to device\n",
    "        with torch.no_grad():  # No backpropagation\n",
    "            outputs = model(bInputIds, attention_mask=bInputMask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()  # Get the predicted labels and move to CPU\n",
    "            predictions.extend(preds)\n",
    "    return predictions\n",
    "\n",
    "# Predict labels for the entire dataset\n",
    "yPred = predictLabels(dataLoader)\n",
    "\n",
    "# Convert labels to CPU for evaluation (if they are on GPU)\n",
    "labels_cpu = labels.cpu().numpy() if labels.is_cuda else labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_cpu, yPred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate misclassification error rate\n",
    "misclassificationErrorRate = 1 - accuracy\n",
    "print(f\"Misclassification Error Rate: {misclassificationErrorRate:.4f}\")\n",
    "\n",
    "# Generate a classification report (precision, recall, F1-score)\n",
    "report = classification_report(labels_cpu, yPred, target_names=['Class 0', 'Class 1'])\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "confMatrix = confusion_matrix(labels_cpu, yPred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confMatrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cbab2-4487-4806-80e8-d002d77a2e73",
   "metadata": {},
   "source": [
    "Bio_ClinicalBERT, a model pre-trained for tasks like language modeling and general understanding of clinical text, lacks a pre-trained classification head necessary for tasks like sequence classification. In this case, the model's classification layer (`classifier.weight` and `classifier.bias`) is randomly initialized, leading to poor performance when applied to a binary classification task without fine-tuning. The model shows a strong imbalance in prediction, performing poorly on Class 0 with low precision, recall, and F1-score, while Class 1 predictions are more accurate but still not optimal. This highlights the need for fine-tuning the classification head to improve its overall accuracy and balance between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7c031-6a14-4142-918b-5ec33d8a7415",
   "metadata": {},
   "source": [
    "## Preliminary fine-tuning of the classification head parameters for gradient optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e215fc-b371-44f4-a467-bc59ab00572c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:09:58.134022Z",
     "iopub.status.busy": "2024-09-10T05:09:58.132382Z",
     "iopub.status.idle": "2024-09-10T05:09:58.142336Z",
     "shell.execute_reply": "2024-09-10T05:09:58.141723Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: bert.embeddings.word_embeddings.weight | Requires Grad: False\n",
      "Parameter: bert.embeddings.position_embeddings.weight | Requires Grad: False\n",
      "Parameter: bert.embeddings.token_type_embeddings.weight | Requires Grad: False\n",
      "Parameter: bert.embeddings.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.embeddings.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.output.dense.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.output.dense.bias | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.weight | Requires Grad: False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.bias | Requires Grad: False\n",
      "Parameter: bert.pooler.dense.weight | Requires Grad: False\n",
      "Parameter: bert.pooler.dense.bias | Requires Grad: False\n",
      "Parameter: classifier.weight | Requires Grad: True\n",
      "Parameter: classifier.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "model.classifier.requires_grad_(True) # Only the gradients of the classification layer should be updated while fine tuning\n",
    "\n",
    "# Print all parameters and their requires_grad status\n",
    "print(\"\\n\".join([f\"Parameter: {name} | Requires Grad: {param.requires_grad}\" for name, param in model.named_parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd98f43f-3191-41a4-90bb-d07da95145b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T05:09:58.148399Z",
     "iopub.status.busy": "2024-09-10T05:09:58.146795Z",
     "iopub.status.idle": "2024-09-11T20:51:39.090879Z",
     "shell.execute_reply": "2024-09-11T20:51:39.090472Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1/2001 completed. Loss: 0.5831\n",
      "Epoch 2/2001 completed. Loss: 0.5043\n",
      "Epoch 3/2001 completed. Loss: 0.4660\n",
      "Epoch 4/2001 completed. Loss: 0.4415\n",
      "Epoch 5/2001 completed. Loss: 0.4202\n",
      "Epoch 6/2001 completed. Loss: 0.4017\n",
      "Epoch 7/2001 completed. Loss: 0.3836\n",
      "Epoch 8/2001 completed. Loss: 0.3656\n",
      "Epoch 9/2001 completed. Loss: 0.3490\n",
      "Epoch 10/2001 completed. Loss: 0.3370\n",
      "Epoch 11/2001 completed. Loss: 0.3214\n",
      "Epoch 12/2001 completed. Loss: 0.3093\n",
      "Epoch 13/2001 completed. Loss: 0.2992\n",
      "Epoch 14/2001 completed. Loss: 0.2880\n",
      "Epoch 15/2001 completed. Loss: 0.2768\n",
      "Epoch 16/2001 completed. Loss: 0.2692\n",
      "Epoch 17/2001 completed. Loss: 0.2565\n",
      "Epoch 18/2001 completed. Loss: 0.2502\n",
      "Epoch 19/2001 completed. Loss: 0.2420\n",
      "Epoch 20/2001 completed. Loss: 0.2359\n",
      "Epoch 21/2001 completed. Loss: 0.2276\n",
      "Epoch 22/2001 completed. Loss: 0.2213\n",
      "Epoch 23/2001 completed. Loss: 0.2140\n",
      "Epoch 24/2001 completed. Loss: 0.2101\n",
      "Epoch 25/2001 completed. Loss: 0.2038\n",
      "Epoch 26/2001 completed. Loss: 0.1984\n",
      "Epoch 27/2001 completed. Loss: 0.1924\n",
      "Epoch 28/2001 completed. Loss: 0.1885\n",
      "Epoch 29/2001 completed. Loss: 0.1839\n",
      "Epoch 30/2001 completed. Loss: 0.1820\n",
      "Epoch 31/2001 completed. Loss: 0.1755\n",
      "Epoch 32/2001 completed. Loss: 0.1724\n",
      "Epoch 33/2001 completed. Loss: 0.1685\n",
      "Epoch 34/2001 completed. Loss: 0.1663\n",
      "Epoch 35/2001 completed. Loss: 0.1624\n",
      "Epoch 36/2001 completed. Loss: 0.1589\n",
      "Epoch 37/2001 completed. Loss: 0.1554\n",
      "Epoch 38/2001 completed. Loss: 0.1537\n",
      "Epoch 39/2001 completed. Loss: 0.1496\n",
      "Epoch 40/2001 completed. Loss: 0.1469\n",
      "Epoch 41/2001 completed. Loss: 0.1455\n",
      "Epoch 42/2001 completed. Loss: 0.1439\n",
      "Epoch 43/2001 completed. Loss: 0.1412\n",
      "Epoch 44/2001 completed. Loss: 0.1382\n",
      "Epoch 45/2001 completed. Loss: 0.1357\n",
      "Epoch 46/2001 completed. Loss: 0.1334\n",
      "Epoch 47/2001 completed. Loss: 0.1328\n",
      "Epoch 48/2001 completed. Loss: 0.1319\n",
      "Epoch 49/2001 completed. Loss: 0.1300\n",
      "Epoch 50/2001 completed. Loss: 0.1280\n",
      "Epoch 51/2001 completed. Loss: 0.1263\n",
      "Epoch 52/2001 completed. Loss: 0.1253\n",
      "Epoch 53/2001 completed. Loss: 0.1228\n",
      "Epoch 54/2001 completed. Loss: 0.1202\n",
      "Epoch 55/2001 completed. Loss: 0.1190\n",
      "Epoch 56/2001 completed. Loss: 0.1181\n",
      "Epoch 57/2001 completed. Loss: 0.1176\n",
      "Epoch 58/2001 completed. Loss: 0.1153\n",
      "Epoch 59/2001 completed. Loss: 0.1127\n",
      "Epoch 60/2001 completed. Loss: 0.1143\n",
      "Epoch 61/2001 completed. Loss: 0.1111\n",
      "Epoch 62/2001 completed. Loss: 0.1121\n",
      "Epoch 63/2001 completed. Loss: 0.1098\n",
      "Epoch 64/2001 completed. Loss: 0.1085\n",
      "Epoch 65/2001 completed. Loss: 0.1074\n",
      "Epoch 66/2001 completed. Loss: 0.1051\n",
      "Epoch 67/2001 completed. Loss: 0.1047\n",
      "Epoch 68/2001 completed. Loss: 0.1046\n",
      "Epoch 69/2001 completed. Loss: 0.1034\n",
      "Epoch 70/2001 completed. Loss: 0.1025\n",
      "Epoch 71/2001 completed. Loss: 0.1021\n",
      "Epoch 72/2001 completed. Loss: 0.1005\n",
      "Epoch 73/2001 completed. Loss: 0.1005\n",
      "Epoch 74/2001 completed. Loss: 0.0992\n",
      "Epoch 75/2001 completed. Loss: 0.0996\n",
      "Epoch 76/2001 completed. Loss: 0.0967\n",
      "Epoch 77/2001 completed. Loss: 0.0979\n",
      "Epoch 78/2001 completed. Loss: 0.0964\n",
      "Epoch 79/2001 completed. Loss: 0.0943\n",
      "Epoch 80/2001 completed. Loss: 0.0958\n",
      "Epoch 81/2001 completed. Loss: 0.0939\n",
      "Epoch 82/2001 completed. Loss: 0.0918\n",
      "Epoch 83/2001 completed. Loss: 0.0921\n",
      "Epoch 84/2001 completed. Loss: 0.0935\n",
      "Epoch 85/2001 completed. Loss: 0.0915\n",
      "Epoch 86/2001 completed. Loss: 0.0891\n",
      "Epoch 87/2001 completed. Loss: 0.0891\n",
      "Epoch 88/2001 completed. Loss: 0.0896\n",
      "Epoch 89/2001 completed. Loss: 0.0887\n",
      "Epoch 90/2001 completed. Loss: 0.0886\n",
      "Epoch 91/2001 completed. Loss: 0.0890\n",
      "Epoch 92/2001 completed. Loss: 0.0879\n",
      "Epoch 93/2001 completed. Loss: 0.0869\n",
      "Epoch 94/2001 completed. Loss: 0.0867\n",
      "Epoch 95/2001 completed. Loss: 0.0861\n",
      "Epoch 96/2001 completed. Loss: 0.0854\n",
      "Epoch 97/2001 completed. Loss: 0.0864\n",
      "Epoch 98/2001 completed. Loss: 0.0845\n",
      "Epoch 99/2001 completed. Loss: 0.0844\n",
      "Epoch 100/2001 completed. Loss: 0.0830\n",
      "Epoch 101/2001 completed. Loss: 0.0828\n",
      "Epoch 102/2001 completed. Loss: 0.0833\n",
      "Epoch 103/2001 completed. Loss: 0.0822\n",
      "Epoch 104/2001 completed. Loss: 0.0824\n",
      "Epoch 105/2001 completed. Loss: 0.0836\n",
      "Epoch 106/2001 completed. Loss: 0.0811\n",
      "Epoch 107/2001 completed. Loss: 0.0816\n",
      "Epoch 108/2001 completed. Loss: 0.0785\n",
      "Epoch 109/2001 completed. Loss: 0.0797\n",
      "Epoch 110/2001 completed. Loss: 0.0801\n",
      "Epoch 111/2001 completed. Loss: 0.0798\n",
      "Epoch 112/2001 completed. Loss: 0.0797\n",
      "Epoch 113/2001 completed. Loss: 0.0788\n",
      "Epoch 114/2001 completed. Loss: 0.0777\n",
      "Epoch 115/2001 completed. Loss: 0.0797\n",
      "Epoch 116/2001 completed. Loss: 0.0793\n",
      "Epoch 117/2001 completed. Loss: 0.0759\n",
      "Epoch 118/2001 completed. Loss: 0.0797\n",
      "Epoch 119/2001 completed. Loss: 0.0764\n",
      "Epoch 120/2001 completed. Loss: 0.0762\n",
      "Epoch 121/2001 completed. Loss: 0.0773\n",
      "Epoch 122/2001 completed. Loss: 0.0769\n",
      "Epoch 123/2001 completed. Loss: 0.0772\n",
      "Epoch 124/2001 completed. Loss: 0.0752\n",
      "Epoch 125/2001 completed. Loss: 0.0754\n",
      "Epoch 126/2001 completed. Loss: 0.0742\n",
      "Epoch 127/2001 completed. Loss: 0.0747\n",
      "Epoch 128/2001 completed. Loss: 0.0762\n",
      "Epoch 129/2001 completed. Loss: 0.0740\n",
      "Epoch 130/2001 completed. Loss: 0.0744\n",
      "Epoch 131/2001 completed. Loss: 0.0743\n",
      "Epoch 132/2001 completed. Loss: 0.0732\n",
      "Epoch 133/2001 completed. Loss: 0.0737\n",
      "Epoch 134/2001 completed. Loss: 0.0763\n",
      "Epoch 135/2001 completed. Loss: 0.0737\n",
      "Epoch 136/2001 completed. Loss: 0.0741\n",
      "Epoch 137/2001 completed. Loss: 0.0719\n",
      "Epoch 138/2001 completed. Loss: 0.0721\n",
      "Epoch 139/2001 completed. Loss: 0.0720\n",
      "Epoch 140/2001 completed. Loss: 0.0715\n",
      "Epoch 141/2001 completed. Loss: 0.0710\n",
      "Epoch 142/2001 completed. Loss: 0.0710\n",
      "Epoch 143/2001 completed. Loss: 0.0720\n",
      "Epoch 144/2001 completed. Loss: 0.0703\n",
      "Epoch 145/2001 completed. Loss: 0.0698\n",
      "Epoch 146/2001 completed. Loss: 0.0701\n",
      "Epoch 147/2001 completed. Loss: 0.0691\n",
      "Epoch 148/2001 completed. Loss: 0.0702\n",
      "Epoch 149/2001 completed. Loss: 0.0706\n",
      "Epoch 150/2001 completed. Loss: 0.0710\n",
      "Epoch 151/2001 completed. Loss: 0.0697\n",
      "Epoch 152/2001 completed. Loss: 0.0706\n",
      "Epoch 153/2001 completed. Loss: 0.0683\n",
      "Epoch 154/2001 completed. Loss: 0.0686\n",
      "Epoch 155/2001 completed. Loss: 0.0681\n",
      "Epoch 156/2001 completed. Loss: 0.0704\n",
      "Epoch 157/2001 completed. Loss: 0.0686\n",
      "Epoch 158/2001 completed. Loss: 0.0678\n",
      "Epoch 159/2001 completed. Loss: 0.0683\n",
      "Epoch 160/2001 completed. Loss: 0.0672\n",
      "Epoch 161/2001 completed. Loss: 0.0685\n",
      "Epoch 162/2001 completed. Loss: 0.0673\n",
      "Epoch 163/2001 completed. Loss: 0.0677\n",
      "Epoch 164/2001 completed. Loss: 0.0685\n",
      "Epoch 165/2001 completed. Loss: 0.0697\n",
      "Epoch 166/2001 completed. Loss: 0.0688\n",
      "Epoch 167/2001 completed. Loss: 0.0670\n",
      "Epoch 168/2001 completed. Loss: 0.0691\n",
      "Epoch 169/2001 completed. Loss: 0.0678\n",
      "Epoch 170/2001 completed. Loss: 0.0676\n",
      "Epoch 171/2001 completed. Loss: 0.0665\n",
      "Epoch 172/2001 completed. Loss: 0.0670\n",
      "Epoch 173/2001 completed. Loss: 0.0659\n",
      "Epoch 174/2001 completed. Loss: 0.0659\n",
      "Epoch 175/2001 completed. Loss: 0.0648\n",
      "Epoch 176/2001 completed. Loss: 0.0652\n",
      "Epoch 177/2001 completed. Loss: 0.0656\n",
      "Epoch 178/2001 completed. Loss: 0.0666\n",
      "Epoch 179/2001 completed. Loss: 0.0667\n",
      "Epoch 180/2001 completed. Loss: 0.0671\n",
      "Epoch 181/2001 completed. Loss: 0.0657\n",
      "Epoch 182/2001 completed. Loss: 0.0654\n",
      "Epoch 183/2001 completed. Loss: 0.0671\n",
      "Epoch 184/2001 completed. Loss: 0.0650\n",
      "Epoch 185/2001 completed. Loss: 0.0659\n",
      "Epoch 186/2001 completed. Loss: 0.0648\n",
      "Epoch 187/2001 completed. Loss: 0.0651\n",
      "Epoch 188/2001 completed. Loss: 0.0639\n",
      "Epoch 189/2001 completed. Loss: 0.0649\n",
      "Epoch 190/2001 completed. Loss: 0.0653\n",
      "Epoch 191/2001 completed. Loss: 0.0634\n",
      "Epoch 192/2001 completed. Loss: 0.0647\n",
      "Epoch 193/2001 completed. Loss: 0.0650\n",
      "Epoch 194/2001 completed. Loss: 0.0651\n",
      "Epoch 195/2001 completed. Loss: 0.0628\n",
      "Epoch 196/2001 completed. Loss: 0.0628\n",
      "Epoch 197/2001 completed. Loss: 0.0641\n",
      "Epoch 198/2001 completed. Loss: 0.0632\n",
      "Epoch 199/2001 completed. Loss: 0.0627\n",
      "Epoch 200/2001 completed. Loss: 0.0616\n",
      "Epoch 201/2001 completed. Loss: 0.0621\n",
      "Epoch 202/2001 completed. Loss: 0.0634\n",
      "Epoch 203/2001 completed. Loss: 0.0624\n",
      "Epoch 204/2001 completed. Loss: 0.0643\n",
      "Epoch 205/2001 completed. Loss: 0.0638\n",
      "Epoch 206/2001 completed. Loss: 0.0645\n",
      "Epoch 207/2001 completed. Loss: 0.0630\n",
      "Epoch 208/2001 completed. Loss: 0.0633\n",
      "Epoch 209/2001 completed. Loss: 0.0627\n",
      "Epoch 210/2001 completed. Loss: 0.0618\n",
      "Epoch 211/2001 completed. Loss: 0.0623\n",
      "Epoch 212/2001 completed. Loss: 0.0631\n",
      "Epoch 213/2001 completed. Loss: 0.0619\n",
      "Epoch 214/2001 completed. Loss: 0.0630\n",
      "Epoch 215/2001 completed. Loss: 0.0609\n",
      "Epoch 216/2001 completed. Loss: 0.0616\n",
      "Epoch 217/2001 completed. Loss: 0.0610\n",
      "Epoch 218/2001 completed. Loss: 0.0612\n",
      "Epoch 219/2001 completed. Loss: 0.0624\n",
      "Epoch 220/2001 completed. Loss: 0.0580\n",
      "Epoch 221/2001 completed. Loss: 0.0616\n",
      "Epoch 222/2001 completed. Loss: 0.0635\n",
      "Epoch 223/2001 completed. Loss: 0.0613\n",
      "Epoch 224/2001 completed. Loss: 0.0597\n",
      "Epoch 225/2001 completed. Loss: 0.0624\n",
      "Epoch 226/2001 completed. Loss: 0.0609\n",
      "Epoch 227/2001 completed. Loss: 0.0616\n",
      "Epoch 228/2001 completed. Loss: 0.0611\n",
      "Epoch 229/2001 completed. Loss: 0.0610\n",
      "Epoch 230/2001 completed. Loss: 0.0603\n",
      "Epoch 231/2001 completed. Loss: 0.0599\n",
      "Epoch 232/2001 completed. Loss: 0.0610\n",
      "Epoch 233/2001 completed. Loss: 0.0592\n",
      "Epoch 234/2001 completed. Loss: 0.0594\n",
      "Epoch 235/2001 completed. Loss: 0.0618\n",
      "Epoch 236/2001 completed. Loss: 0.0602\n",
      "Epoch 237/2001 completed. Loss: 0.0599\n",
      "Epoch 238/2001 completed. Loss: 0.0592\n",
      "Epoch 239/2001 completed. Loss: 0.0597\n",
      "Epoch 240/2001 completed. Loss: 0.0591\n",
      "Epoch 241/2001 completed. Loss: 0.0580\n",
      "Epoch 242/2001 completed. Loss: 0.0616\n",
      "Epoch 243/2001 completed. Loss: 0.0593\n",
      "Epoch 244/2001 completed. Loss: 0.0601\n",
      "Epoch 245/2001 completed. Loss: 0.0600\n",
      "Epoch 246/2001 completed. Loss: 0.0595\n",
      "Epoch 247/2001 completed. Loss: 0.0594\n",
      "Epoch 248/2001 completed. Loss: 0.0590\n",
      "Epoch 249/2001 completed. Loss: 0.0599\n",
      "Epoch 250/2001 completed. Loss: 0.0586\n",
      "Epoch 251/2001 completed. Loss: 0.0591\n",
      "Epoch 252/2001 completed. Loss: 0.0598\n",
      "Epoch 253/2001 completed. Loss: 0.0586\n",
      "Epoch 254/2001 completed. Loss: 0.0595\n",
      "Epoch 255/2001 completed. Loss: 0.0579\n",
      "Epoch 256/2001 completed. Loss: 0.0587\n",
      "Epoch 257/2001 completed. Loss: 0.0579\n",
      "Epoch 258/2001 completed. Loss: 0.0588\n",
      "Epoch 259/2001 completed. Loss: 0.0597\n",
      "Epoch 260/2001 completed. Loss: 0.0596\n",
      "Epoch 261/2001 completed. Loss: 0.0588\n",
      "Epoch 262/2001 completed. Loss: 0.0597\n",
      "Epoch 263/2001 completed. Loss: 0.0585\n",
      "Epoch 264/2001 completed. Loss: 0.0577\n",
      "Epoch 265/2001 completed. Loss: 0.0573\n",
      "Epoch 266/2001 completed. Loss: 0.0575\n",
      "Epoch 267/2001 completed. Loss: 0.0568\n",
      "Epoch 268/2001 completed. Loss: 0.0580\n",
      "Epoch 269/2001 completed. Loss: 0.0581\n",
      "Epoch 270/2001 completed. Loss: 0.0579\n",
      "Epoch 271/2001 completed. Loss: 0.0582\n",
      "Epoch 272/2001 completed. Loss: 0.0585\n",
      "Epoch 273/2001 completed. Loss: 0.0586\n",
      "Epoch 274/2001 completed. Loss: 0.0577\n",
      "Epoch 275/2001 completed. Loss: 0.0563\n",
      "Epoch 276/2001 completed. Loss: 0.0590\n",
      "Epoch 277/2001 completed. Loss: 0.0558\n",
      "Epoch 278/2001 completed. Loss: 0.0567\n",
      "Epoch 279/2001 completed. Loss: 0.0589\n",
      "Epoch 280/2001 completed. Loss: 0.0578\n",
      "Epoch 281/2001 completed. Loss: 0.0566\n",
      "Epoch 282/2001 completed. Loss: 0.0556\n",
      "Epoch 283/2001 completed. Loss: 0.0573\n",
      "Epoch 284/2001 completed. Loss: 0.0566\n",
      "Epoch 285/2001 completed. Loss: 0.0569\n",
      "Epoch 286/2001 completed. Loss: 0.0569\n",
      "Epoch 287/2001 completed. Loss: 0.0573\n",
      "Epoch 288/2001 completed. Loss: 0.0576\n",
      "Epoch 289/2001 completed. Loss: 0.0568\n",
      "Epoch 290/2001 completed. Loss: 0.0575\n",
      "Epoch 291/2001 completed. Loss: 0.0573\n",
      "Epoch 292/2001 completed. Loss: 0.0566\n",
      "Epoch 293/2001 completed. Loss: 0.0559\n",
      "Epoch 294/2001 completed. Loss: 0.0578\n",
      "Epoch 295/2001 completed. Loss: 0.0562\n",
      "Epoch 296/2001 completed. Loss: 0.0556\n",
      "Epoch 297/2001 completed. Loss: 0.0562\n",
      "Epoch 298/2001 completed. Loss: 0.0563\n",
      "Epoch 299/2001 completed. Loss: 0.0568\n",
      "Epoch 300/2001 completed. Loss: 0.0570\n",
      "Epoch 301/2001 completed. Loss: 0.0565\n",
      "Epoch 302/2001 completed. Loss: 0.0571\n",
      "Epoch 303/2001 completed. Loss: 0.0567\n",
      "Epoch 304/2001 completed. Loss: 0.0587\n",
      "Epoch 305/2001 completed. Loss: 0.0554\n",
      "Epoch 306/2001 completed. Loss: 0.0577\n",
      "Epoch 307/2001 completed. Loss: 0.0567\n",
      "Epoch 308/2001 completed. Loss: 0.0549\n",
      "Epoch 309/2001 completed. Loss: 0.0570\n",
      "Epoch 310/2001 completed. Loss: 0.0551\n",
      "Epoch 311/2001 completed. Loss: 0.0575\n",
      "Epoch 312/2001 completed. Loss: 0.0564\n",
      "Epoch 313/2001 completed. Loss: 0.0566\n",
      "Epoch 314/2001 completed. Loss: 0.0548\n",
      "Epoch 315/2001 completed. Loss: 0.0565\n",
      "Epoch 316/2001 completed. Loss: 0.0558\n",
      "Epoch 317/2001 completed. Loss: 0.0539\n",
      "Epoch 318/2001 completed. Loss: 0.0573\n",
      "Epoch 319/2001 completed. Loss: 0.0552\n",
      "Epoch 320/2001 completed. Loss: 0.0534\n",
      "Epoch 321/2001 completed. Loss: 0.0563\n",
      "Epoch 322/2001 completed. Loss: 0.0528\n",
      "Epoch 323/2001 completed. Loss: 0.0552\n",
      "Epoch 324/2001 completed. Loss: 0.0534\n",
      "Epoch 325/2001 completed. Loss: 0.0542\n",
      "Epoch 326/2001 completed. Loss: 0.0547\n",
      "Epoch 327/2001 completed. Loss: 0.0543\n",
      "Epoch 328/2001 completed. Loss: 0.0552\n",
      "Epoch 329/2001 completed. Loss: 0.0571\n",
      "Epoch 330/2001 completed. Loss: 0.0559\n",
      "Epoch 331/2001 completed. Loss: 0.0548\n",
      "Epoch 332/2001 completed. Loss: 0.0536\n",
      "Epoch 333/2001 completed. Loss: 0.0534\n",
      "Epoch 334/2001 completed. Loss: 0.0528\n",
      "Epoch 335/2001 completed. Loss: 0.0551\n",
      "Epoch 336/2001 completed. Loss: 0.0537\n",
      "Epoch 337/2001 completed. Loss: 0.0561\n",
      "Epoch 338/2001 completed. Loss: 0.0539\n",
      "Epoch 339/2001 completed. Loss: 0.0552\n",
      "Epoch 340/2001 completed. Loss: 0.0539\n",
      "Epoch 341/2001 completed. Loss: 0.0548\n",
      "Epoch 342/2001 completed. Loss: 0.0546\n",
      "Epoch 343/2001 completed. Loss: 0.0534\n",
      "Epoch 344/2001 completed. Loss: 0.0559\n",
      "Epoch 345/2001 completed. Loss: 0.0564\n",
      "Epoch 346/2001 completed. Loss: 0.0546\n",
      "Epoch 347/2001 completed. Loss: 0.0532\n",
      "Epoch 348/2001 completed. Loss: 0.0545\n",
      "Epoch 349/2001 completed. Loss: 0.0538\n",
      "Epoch 350/2001 completed. Loss: 0.0518\n",
      "Epoch 351/2001 completed. Loss: 0.0535\n",
      "Epoch 352/2001 completed. Loss: 0.0523\n",
      "Epoch 353/2001 completed. Loss: 0.0537\n",
      "Epoch 354/2001 completed. Loss: 0.0535\n",
      "Epoch 355/2001 completed. Loss: 0.0549\n",
      "Epoch 356/2001 completed. Loss: 0.0530\n",
      "Epoch 357/2001 completed. Loss: 0.0550\n",
      "Epoch 358/2001 completed. Loss: 0.0528\n",
      "Epoch 359/2001 completed. Loss: 0.0534\n",
      "Epoch 360/2001 completed. Loss: 0.0526\n",
      "Epoch 361/2001 completed. Loss: 0.0542\n",
      "Epoch 362/2001 completed. Loss: 0.0544\n",
      "Epoch 363/2001 completed. Loss: 0.0543\n",
      "Epoch 364/2001 completed. Loss: 0.0540\n",
      "Epoch 365/2001 completed. Loss: 0.0517\n",
      "Epoch 366/2001 completed. Loss: 0.0534\n",
      "Epoch 367/2001 completed. Loss: 0.0529\n",
      "Epoch 368/2001 completed. Loss: 0.0539\n",
      "Epoch 369/2001 completed. Loss: 0.0531\n",
      "Epoch 370/2001 completed. Loss: 0.0539\n",
      "Epoch 371/2001 completed. Loss: 0.0541\n",
      "Epoch 372/2001 completed. Loss: 0.0523\n",
      "Epoch 373/2001 completed. Loss: 0.0545\n",
      "Epoch 374/2001 completed. Loss: 0.0518\n",
      "Epoch 375/2001 completed. Loss: 0.0533\n",
      "Epoch 376/2001 completed. Loss: 0.0507\n",
      "Epoch 377/2001 completed. Loss: 0.0521\n",
      "Epoch 378/2001 completed. Loss: 0.0538\n",
      "Epoch 379/2001 completed. Loss: 0.0519\n",
      "Epoch 380/2001 completed. Loss: 0.0532\n",
      "Epoch 381/2001 completed. Loss: 0.0524\n",
      "Epoch 382/2001 completed. Loss: 0.0524\n",
      "Epoch 383/2001 completed. Loss: 0.0517\n",
      "Epoch 384/2001 completed. Loss: 0.0522\n",
      "Epoch 385/2001 completed. Loss: 0.0529\n",
      "Epoch 386/2001 completed. Loss: 0.0552\n",
      "Epoch 387/2001 completed. Loss: 0.0540\n",
      "Epoch 388/2001 completed. Loss: 0.0542\n",
      "Epoch 389/2001 completed. Loss: 0.0514\n",
      "Epoch 390/2001 completed. Loss: 0.0529\n",
      "Epoch 391/2001 completed. Loss: 0.0523\n",
      "Epoch 392/2001 completed. Loss: 0.0521\n",
      "Epoch 393/2001 completed. Loss: 0.0540\n",
      "Epoch 394/2001 completed. Loss: 0.0532\n",
      "Epoch 395/2001 completed. Loss: 0.0524\n",
      "Epoch 396/2001 completed. Loss: 0.0519\n",
      "Epoch 397/2001 completed. Loss: 0.0542\n",
      "Epoch 398/2001 completed. Loss: 0.0522\n",
      "Epoch 399/2001 completed. Loss: 0.0525\n",
      "Epoch 400/2001 completed. Loss: 0.0509\n",
      "Epoch 401/2001 completed. Loss: 0.0534\n",
      "Epoch 402/2001 completed. Loss: 0.0539\n",
      "Epoch 403/2001 completed. Loss: 0.0531\n",
      "Epoch 404/2001 completed. Loss: 0.0514\n",
      "Epoch 405/2001 completed. Loss: 0.0520\n",
      "Epoch 406/2001 completed. Loss: 0.0524\n",
      "Epoch 407/2001 completed. Loss: 0.0525\n",
      "Epoch 408/2001 completed. Loss: 0.0508\n",
      "Epoch 409/2001 completed. Loss: 0.0524\n",
      "Epoch 410/2001 completed. Loss: 0.0507\n",
      "Epoch 411/2001 completed. Loss: 0.0532\n",
      "Epoch 412/2001 completed. Loss: 0.0523\n",
      "Epoch 413/2001 completed. Loss: 0.0510\n",
      "Epoch 414/2001 completed. Loss: 0.0501\n",
      "Epoch 415/2001 completed. Loss: 0.0501\n",
      "Epoch 416/2001 completed. Loss: 0.0514\n",
      "Epoch 417/2001 completed. Loss: 0.0520\n",
      "Epoch 418/2001 completed. Loss: 0.0512\n",
      "Epoch 419/2001 completed. Loss: 0.0530\n",
      "Epoch 420/2001 completed. Loss: 0.0517\n",
      "Epoch 421/2001 completed. Loss: 0.0521\n",
      "Epoch 422/2001 completed. Loss: 0.0520\n",
      "Epoch 423/2001 completed. Loss: 0.0529\n",
      "Epoch 424/2001 completed. Loss: 0.0525\n",
      "Epoch 425/2001 completed. Loss: 0.0509\n",
      "Epoch 426/2001 completed. Loss: 0.0513\n",
      "Epoch 427/2001 completed. Loss: 0.0510\n",
      "Epoch 428/2001 completed. Loss: 0.0499\n",
      "Epoch 429/2001 completed. Loss: 0.0512\n",
      "Epoch 430/2001 completed. Loss: 0.0527\n",
      "Epoch 431/2001 completed. Loss: 0.0513\n",
      "Epoch 432/2001 completed. Loss: 0.0508\n",
      "Epoch 433/2001 completed. Loss: 0.0511\n",
      "Epoch 434/2001 completed. Loss: 0.0500\n",
      "Epoch 435/2001 completed. Loss: 0.0516\n",
      "Epoch 436/2001 completed. Loss: 0.0513\n",
      "Epoch 437/2001 completed. Loss: 0.0503\n",
      "Epoch 438/2001 completed. Loss: 0.0514\n",
      "Epoch 439/2001 completed. Loss: 0.0531\n",
      "Epoch 440/2001 completed. Loss: 0.0497\n",
      "Epoch 441/2001 completed. Loss: 0.0498\n",
      "Epoch 442/2001 completed. Loss: 0.0520\n",
      "Epoch 443/2001 completed. Loss: 0.0505\n",
      "Epoch 444/2001 completed. Loss: 0.0517\n",
      "Epoch 445/2001 completed. Loss: 0.0503\n",
      "Epoch 446/2001 completed. Loss: 0.0506\n",
      "Epoch 447/2001 completed. Loss: 0.0507\n",
      "Epoch 448/2001 completed. Loss: 0.0504\n",
      "Epoch 449/2001 completed. Loss: 0.0501\n",
      "Epoch 450/2001 completed. Loss: 0.0517\n",
      "Epoch 451/2001 completed. Loss: 0.0492\n",
      "Epoch 452/2001 completed. Loss: 0.0494\n",
      "Epoch 453/2001 completed. Loss: 0.0498\n",
      "Epoch 454/2001 completed. Loss: 0.0505\n",
      "Epoch 455/2001 completed. Loss: 0.0509\n",
      "Epoch 456/2001 completed. Loss: 0.0509\n",
      "Epoch 457/2001 completed. Loss: 0.0499\n",
      "Epoch 458/2001 completed. Loss: 0.0500\n",
      "Epoch 459/2001 completed. Loss: 0.0494\n",
      "Epoch 460/2001 completed. Loss: 0.0507\n",
      "Epoch 461/2001 completed. Loss: 0.0503\n",
      "Epoch 462/2001 completed. Loss: 0.0499\n",
      "Epoch 463/2001 completed. Loss: 0.0493\n",
      "Epoch 464/2001 completed. Loss: 0.0487\n",
      "Epoch 465/2001 completed. Loss: 0.0504\n",
      "Epoch 466/2001 completed. Loss: 0.0502\n",
      "Epoch 467/2001 completed. Loss: 0.0499\n",
      "Epoch 468/2001 completed. Loss: 0.0494\n",
      "Epoch 469/2001 completed. Loss: 0.0496\n",
      "Epoch 470/2001 completed. Loss: 0.0505\n",
      "Epoch 471/2001 completed. Loss: 0.0504\n",
      "Epoch 472/2001 completed. Loss: 0.0501\n",
      "Epoch 473/2001 completed. Loss: 0.0505\n",
      "Epoch 474/2001 completed. Loss: 0.0499\n",
      "Epoch 475/2001 completed. Loss: 0.0507\n",
      "Epoch 476/2001 completed. Loss: 0.0488\n",
      "Epoch 477/2001 completed. Loss: 0.0514\n",
      "Epoch 478/2001 completed. Loss: 0.0491\n",
      "Epoch 479/2001 completed. Loss: 0.0490\n",
      "Epoch 480/2001 completed. Loss: 0.0498\n",
      "Epoch 481/2001 completed. Loss: 0.0517\n",
      "Epoch 482/2001 completed. Loss: 0.0483\n",
      "Epoch 483/2001 completed. Loss: 0.0500\n",
      "Epoch 484/2001 completed. Loss: 0.0499\n",
      "Epoch 485/2001 completed. Loss: 0.0481\n",
      "Epoch 486/2001 completed. Loss: 0.0492\n",
      "Epoch 487/2001 completed. Loss: 0.0503\n",
      "Epoch 488/2001 completed. Loss: 0.0495\n",
      "Epoch 489/2001 completed. Loss: 0.0501\n",
      "Epoch 490/2001 completed. Loss: 0.0502\n",
      "Epoch 491/2001 completed. Loss: 0.0497\n",
      "Epoch 492/2001 completed. Loss: 0.0485\n",
      "Epoch 493/2001 completed. Loss: 0.0489\n",
      "Epoch 494/2001 completed. Loss: 0.0500\n",
      "Epoch 495/2001 completed. Loss: 0.0508\n",
      "Epoch 496/2001 completed. Loss: 0.0499\n",
      "Epoch 497/2001 completed. Loss: 0.0481\n",
      "Epoch 498/2001 completed. Loss: 0.0497\n",
      "Epoch 499/2001 completed. Loss: 0.0497\n",
      "Epoch 500/2001 completed. Loss: 0.0512\n",
      "Epoch 501/2001 completed. Loss: 0.0492\n",
      "Epoch 502/2001 completed. Loss: 0.0505\n",
      "Epoch 503/2001 completed. Loss: 0.0473\n",
      "Epoch 504/2001 completed. Loss: 0.0504\n",
      "Epoch 505/2001 completed. Loss: 0.0495\n",
      "Epoch 506/2001 completed. Loss: 0.0500\n",
      "Epoch 507/2001 completed. Loss: 0.0486\n",
      "Epoch 508/2001 completed. Loss: 0.0499\n",
      "Epoch 509/2001 completed. Loss: 0.0481\n",
      "Epoch 510/2001 completed. Loss: 0.0510\n",
      "Epoch 511/2001 completed. Loss: 0.0491\n",
      "Epoch 512/2001 completed. Loss: 0.0481\n",
      "Epoch 513/2001 completed. Loss: 0.0493\n",
      "Epoch 514/2001 completed. Loss: 0.0489\n",
      "Epoch 515/2001 completed. Loss: 0.0475\n",
      "Epoch 516/2001 completed. Loss: 0.0491\n",
      "Epoch 517/2001 completed. Loss: 0.0489\n",
      "Epoch 518/2001 completed. Loss: 0.0492\n",
      "Epoch 519/2001 completed. Loss: 0.0494\n",
      "Epoch 520/2001 completed. Loss: 0.0481\n",
      "Epoch 521/2001 completed. Loss: 0.0490\n",
      "Epoch 522/2001 completed. Loss: 0.0473\n",
      "Epoch 523/2001 completed. Loss: 0.0498\n",
      "Epoch 524/2001 completed. Loss: 0.0499\n",
      "Epoch 525/2001 completed. Loss: 0.0501\n",
      "Epoch 526/2001 completed. Loss: 0.0469\n",
      "Epoch 527/2001 completed. Loss: 0.0480\n",
      "Epoch 528/2001 completed. Loss: 0.0475\n",
      "Epoch 529/2001 completed. Loss: 0.0496\n",
      "Epoch 530/2001 completed. Loss: 0.0497\n",
      "Epoch 531/2001 completed. Loss: 0.0501\n",
      "Epoch 532/2001 completed. Loss: 0.0470\n",
      "Epoch 533/2001 completed. Loss: 0.0506\n",
      "Epoch 534/2001 completed. Loss: 0.0494\n",
      "Epoch 535/2001 completed. Loss: 0.0499\n",
      "Epoch 536/2001 completed. Loss: 0.0497\n",
      "Epoch 537/2001 completed. Loss: 0.0484\n",
      "Epoch 538/2001 completed. Loss: 0.0488\n",
      "Epoch 539/2001 completed. Loss: 0.0497\n",
      "Epoch 540/2001 completed. Loss: 0.0495\n",
      "Epoch 541/2001 completed. Loss: 0.0492\n",
      "Epoch 542/2001 completed. Loss: 0.0500\n",
      "Epoch 543/2001 completed. Loss: 0.0475\n",
      "Epoch 544/2001 completed. Loss: 0.0483\n",
      "Epoch 545/2001 completed. Loss: 0.0491\n",
      "Epoch 546/2001 completed. Loss: 0.0491\n",
      "Epoch 547/2001 completed. Loss: 0.0487\n",
      "Epoch 548/2001 completed. Loss: 0.0463\n",
      "Epoch 549/2001 completed. Loss: 0.0494\n",
      "Epoch 550/2001 completed. Loss: 0.0478\n",
      "Epoch 551/2001 completed. Loss: 0.0490\n",
      "Epoch 552/2001 completed. Loss: 0.0477\n",
      "Epoch 553/2001 completed. Loss: 0.0460\n",
      "Epoch 554/2001 completed. Loss: 0.0470\n",
      "Epoch 555/2001 completed. Loss: 0.0495\n",
      "Epoch 556/2001 completed. Loss: 0.0479\n",
      "Epoch 557/2001 completed. Loss: 0.0483\n",
      "Epoch 558/2001 completed. Loss: 0.0473\n",
      "Epoch 559/2001 completed. Loss: 0.0473\n",
      "Epoch 560/2001 completed. Loss: 0.0485\n",
      "Epoch 561/2001 completed. Loss: 0.0496\n",
      "Epoch 562/2001 completed. Loss: 0.0481\n",
      "Epoch 563/2001 completed. Loss: 0.0470\n",
      "Epoch 564/2001 completed. Loss: 0.0481\n",
      "Epoch 565/2001 completed. Loss: 0.0478\n",
      "Epoch 566/2001 completed. Loss: 0.0483\n",
      "Epoch 567/2001 completed. Loss: 0.0490\n",
      "Epoch 568/2001 completed. Loss: 0.0488\n",
      "Epoch 569/2001 completed. Loss: 0.0468\n",
      "Epoch 570/2001 completed. Loss: 0.0467\n",
      "Epoch 571/2001 completed. Loss: 0.0469\n",
      "Epoch 572/2001 completed. Loss: 0.0491\n",
      "Epoch 573/2001 completed. Loss: 0.0496\n",
      "Epoch 574/2001 completed. Loss: 0.0484\n",
      "Epoch 575/2001 completed. Loss: 0.0482\n",
      "Epoch 576/2001 completed. Loss: 0.0485\n",
      "Epoch 577/2001 completed. Loss: 0.0487\n",
      "Epoch 578/2001 completed. Loss: 0.0480\n",
      "Epoch 579/2001 completed. Loss: 0.0472\n",
      "Epoch 580/2001 completed. Loss: 0.0482\n",
      "Epoch 581/2001 completed. Loss: 0.0459\n",
      "Epoch 582/2001 completed. Loss: 0.0474\n",
      "Epoch 583/2001 completed. Loss: 0.0473\n",
      "Epoch 584/2001 completed. Loss: 0.0484\n",
      "Epoch 585/2001 completed. Loss: 0.0471\n",
      "Epoch 586/2001 completed. Loss: 0.0474\n",
      "Epoch 587/2001 completed. Loss: 0.0487\n",
      "Epoch 588/2001 completed. Loss: 0.0461\n",
      "Epoch 589/2001 completed. Loss: 0.0468\n",
      "Epoch 590/2001 completed. Loss: 0.0488\n",
      "Epoch 591/2001 completed. Loss: 0.0482\n",
      "Epoch 592/2001 completed. Loss: 0.0471\n",
      "Epoch 593/2001 completed. Loss: 0.0484\n",
      "Epoch 594/2001 completed. Loss: 0.0461\n",
      "Epoch 595/2001 completed. Loss: 0.0461\n",
      "Epoch 596/2001 completed. Loss: 0.0468\n",
      "Epoch 597/2001 completed. Loss: 0.0483\n",
      "Epoch 598/2001 completed. Loss: 0.0452\n",
      "Epoch 599/2001 completed. Loss: 0.0485\n",
      "Epoch 600/2001 completed. Loss: 0.0484\n",
      "Epoch 601/2001 completed. Loss: 0.0472\n",
      "Epoch 602/2001 completed. Loss: 0.0489\n",
      "Epoch 603/2001 completed. Loss: 0.0479\n",
      "Epoch 604/2001 completed. Loss: 0.0492\n",
      "Epoch 605/2001 completed. Loss: 0.0486\n",
      "Epoch 606/2001 completed. Loss: 0.0488\n",
      "Epoch 607/2001 completed. Loss: 0.0467\n",
      "Epoch 608/2001 completed. Loss: 0.0478\n",
      "Epoch 609/2001 completed. Loss: 0.0477\n",
      "Epoch 610/2001 completed. Loss: 0.0466\n",
      "Epoch 611/2001 completed. Loss: 0.0465\n",
      "Epoch 612/2001 completed. Loss: 0.0462\n",
      "Epoch 613/2001 completed. Loss: 0.0480\n",
      "Epoch 614/2001 completed. Loss: 0.0469\n",
      "Epoch 615/2001 completed. Loss: 0.0472\n",
      "Epoch 616/2001 completed. Loss: 0.0479\n",
      "Epoch 617/2001 completed. Loss: 0.0479\n",
      "Epoch 618/2001 completed. Loss: 0.0479\n",
      "Epoch 619/2001 completed. Loss: 0.0476\n",
      "Epoch 620/2001 completed. Loss: 0.0476\n",
      "Epoch 621/2001 completed. Loss: 0.0481\n",
      "Epoch 622/2001 completed. Loss: 0.0468\n",
      "Epoch 623/2001 completed. Loss: 0.0471\n",
      "Epoch 624/2001 completed. Loss: 0.0475\n",
      "Epoch 625/2001 completed. Loss: 0.0471\n",
      "Epoch 626/2001 completed. Loss: 0.0453\n",
      "Epoch 627/2001 completed. Loss: 0.0457\n",
      "Epoch 628/2001 completed. Loss: 0.0441\n",
      "Epoch 629/2001 completed. Loss: 0.0477\n",
      "Epoch 630/2001 completed. Loss: 0.0477\n",
      "Epoch 631/2001 completed. Loss: 0.0469\n",
      "Epoch 632/2001 completed. Loss: 0.0465\n",
      "Epoch 633/2001 completed. Loss: 0.0477\n",
      "Epoch 634/2001 completed. Loss: 0.0465\n",
      "Epoch 635/2001 completed. Loss: 0.0446\n",
      "Epoch 636/2001 completed. Loss: 0.0446\n",
      "Epoch 637/2001 completed. Loss: 0.0467\n",
      "Epoch 638/2001 completed. Loss: 0.0447\n",
      "Epoch 639/2001 completed. Loss: 0.0482\n",
      "Epoch 640/2001 completed. Loss: 0.0456\n",
      "Epoch 641/2001 completed. Loss: 0.0492\n",
      "Epoch 642/2001 completed. Loss: 0.0464\n",
      "Epoch 643/2001 completed. Loss: 0.0470\n",
      "Epoch 644/2001 completed. Loss: 0.0467\n",
      "Epoch 645/2001 completed. Loss: 0.0491\n",
      "Epoch 646/2001 completed. Loss: 0.0463\n",
      "Epoch 647/2001 completed. Loss: 0.0453\n",
      "Epoch 648/2001 completed. Loss: 0.0482\n",
      "Epoch 649/2001 completed. Loss: 0.0455\n",
      "Epoch 650/2001 completed. Loss: 0.0469\n",
      "Epoch 651/2001 completed. Loss: 0.0463\n",
      "Epoch 652/2001 completed. Loss: 0.0478\n",
      "Epoch 653/2001 completed. Loss: 0.0465\n",
      "Epoch 654/2001 completed. Loss: 0.0463\n",
      "Epoch 655/2001 completed. Loss: 0.0467\n",
      "Epoch 656/2001 completed. Loss: 0.0437\n",
      "Epoch 657/2001 completed. Loss: 0.0461\n",
      "Epoch 658/2001 completed. Loss: 0.0463\n",
      "Epoch 659/2001 completed. Loss: 0.0464\n",
      "Epoch 660/2001 completed. Loss: 0.0464\n",
      "Epoch 661/2001 completed. Loss: 0.0473\n",
      "Epoch 662/2001 completed. Loss: 0.0485\n",
      "Epoch 663/2001 completed. Loss: 0.0462\n",
      "Epoch 664/2001 completed. Loss: 0.0461\n",
      "Epoch 665/2001 completed. Loss: 0.0464\n",
      "Epoch 666/2001 completed. Loss: 0.0481\n",
      "Epoch 667/2001 completed. Loss: 0.0444\n",
      "Epoch 668/2001 completed. Loss: 0.0453\n",
      "Epoch 669/2001 completed. Loss: 0.0453\n",
      "Epoch 670/2001 completed. Loss: 0.0458\n",
      "Epoch 671/2001 completed. Loss: 0.0436\n",
      "Epoch 672/2001 completed. Loss: 0.0454\n",
      "Epoch 673/2001 completed. Loss: 0.0473\n",
      "Epoch 674/2001 completed. Loss: 0.0459\n",
      "Epoch 675/2001 completed. Loss: 0.0461\n",
      "Epoch 676/2001 completed. Loss: 0.0466\n",
      "Epoch 677/2001 completed. Loss: 0.0467\n",
      "Epoch 678/2001 completed. Loss: 0.0477\n",
      "Epoch 679/2001 completed. Loss: 0.0462\n",
      "Epoch 680/2001 completed. Loss: 0.0456\n",
      "Epoch 681/2001 completed. Loss: 0.0458\n",
      "Epoch 682/2001 completed. Loss: 0.0463\n",
      "Epoch 683/2001 completed. Loss: 0.0459\n",
      "Epoch 684/2001 completed. Loss: 0.0438\n",
      "Epoch 685/2001 completed. Loss: 0.0456\n",
      "Epoch 686/2001 completed. Loss: 0.0466\n",
      "Epoch 687/2001 completed. Loss: 0.0470\n",
      "Epoch 688/2001 completed. Loss: 0.0463\n",
      "Epoch 689/2001 completed. Loss: 0.0458\n",
      "Epoch 690/2001 completed. Loss: 0.0484\n",
      "Epoch 691/2001 completed. Loss: 0.0451\n",
      "Epoch 692/2001 completed. Loss: 0.0467\n",
      "Epoch 693/2001 completed. Loss: 0.0467\n",
      "Epoch 694/2001 completed. Loss: 0.0458\n",
      "Epoch 695/2001 completed. Loss: 0.0467\n",
      "Epoch 696/2001 completed. Loss: 0.0447\n",
      "Epoch 697/2001 completed. Loss: 0.0450\n",
      "Epoch 698/2001 completed. Loss: 0.0467\n",
      "Epoch 699/2001 completed. Loss: 0.0437\n",
      "Epoch 700/2001 completed. Loss: 0.0457\n",
      "Epoch 701/2001 completed. Loss: 0.0464\n",
      "Epoch 702/2001 completed. Loss: 0.0458\n",
      "Epoch 703/2001 completed. Loss: 0.0454\n",
      "Epoch 704/2001 completed. Loss: 0.0455\n",
      "Epoch 705/2001 completed. Loss: 0.0462\n",
      "Epoch 706/2001 completed. Loss: 0.0455\n",
      "Epoch 707/2001 completed. Loss: 0.0449\n",
      "Epoch 708/2001 completed. Loss: 0.0435\n",
      "Epoch 709/2001 completed. Loss: 0.0452\n",
      "Epoch 710/2001 completed. Loss: 0.0441\n",
      "Epoch 711/2001 completed. Loss: 0.0451\n",
      "Epoch 712/2001 completed. Loss: 0.0434\n",
      "Epoch 713/2001 completed. Loss: 0.0452\n",
      "Epoch 714/2001 completed. Loss: 0.0446\n",
      "Epoch 715/2001 completed. Loss: 0.0443\n",
      "Epoch 716/2001 completed. Loss: 0.0462\n",
      "Epoch 717/2001 completed. Loss: 0.0455\n",
      "Epoch 718/2001 completed. Loss: 0.0459\n",
      "Epoch 719/2001 completed. Loss: 0.0449\n",
      "Epoch 720/2001 completed. Loss: 0.0453\n",
      "Epoch 721/2001 completed. Loss: 0.0459\n",
      "Epoch 722/2001 completed. Loss: 0.0444\n",
      "Epoch 723/2001 completed. Loss: 0.0435\n",
      "Epoch 724/2001 completed. Loss: 0.0444\n",
      "Epoch 725/2001 completed. Loss: 0.0434\n",
      "Epoch 726/2001 completed. Loss: 0.0453\n",
      "Epoch 727/2001 completed. Loss: 0.0438\n",
      "Epoch 728/2001 completed. Loss: 0.0458\n",
      "Epoch 729/2001 completed. Loss: 0.0466\n",
      "Epoch 730/2001 completed. Loss: 0.0461\n",
      "Epoch 731/2001 completed. Loss: 0.0449\n",
      "Epoch 732/2001 completed. Loss: 0.0479\n",
      "Epoch 733/2001 completed. Loss: 0.0446\n",
      "Epoch 734/2001 completed. Loss: 0.0460\n",
      "Epoch 735/2001 completed. Loss: 0.0448\n",
      "Epoch 736/2001 completed. Loss: 0.0452\n",
      "Epoch 737/2001 completed. Loss: 0.0462\n",
      "Epoch 738/2001 completed. Loss: 0.0460\n",
      "Epoch 739/2001 completed. Loss: 0.0458\n",
      "Epoch 740/2001 completed. Loss: 0.0458\n",
      "Epoch 741/2001 completed. Loss: 0.0448\n",
      "Epoch 742/2001 completed. Loss: 0.0434\n",
      "Epoch 743/2001 completed. Loss: 0.0459\n",
      "Epoch 744/2001 completed. Loss: 0.0449\n",
      "Epoch 745/2001 completed. Loss: 0.0466\n",
      "Epoch 746/2001 completed. Loss: 0.0445\n",
      "Epoch 747/2001 completed. Loss: 0.0448\n",
      "Epoch 748/2001 completed. Loss: 0.0462\n",
      "Epoch 749/2001 completed. Loss: 0.0465\n",
      "Epoch 750/2001 completed. Loss: 0.0470\n",
      "Epoch 751/2001 completed. Loss: 0.0476\n",
      "Epoch 752/2001 completed. Loss: 0.0432\n",
      "Epoch 753/2001 completed. Loss: 0.0440\n",
      "Epoch 754/2001 completed. Loss: 0.0455\n",
      "Epoch 755/2001 completed. Loss: 0.0454\n",
      "Epoch 756/2001 completed. Loss: 0.0433\n",
      "Epoch 757/2001 completed. Loss: 0.0448\n",
      "Epoch 758/2001 completed. Loss: 0.0438\n",
      "Epoch 759/2001 completed. Loss: 0.0443\n",
      "Epoch 760/2001 completed. Loss: 0.0445\n",
      "Epoch 761/2001 completed. Loss: 0.0434\n",
      "Epoch 762/2001 completed. Loss: 0.0456\n",
      "Epoch 763/2001 completed. Loss: 0.0449\n",
      "Epoch 764/2001 completed. Loss: 0.0427\n",
      "Epoch 765/2001 completed. Loss: 0.0440\n",
      "Epoch 766/2001 completed. Loss: 0.0443\n",
      "Epoch 767/2001 completed. Loss: 0.0435\n",
      "Epoch 768/2001 completed. Loss: 0.0437\n",
      "Epoch 769/2001 completed. Loss: 0.0440\n",
      "Epoch 770/2001 completed. Loss: 0.0448\n",
      "Epoch 771/2001 completed. Loss: 0.0453\n",
      "Epoch 772/2001 completed. Loss: 0.0444\n",
      "Epoch 773/2001 completed. Loss: 0.0442\n",
      "Epoch 774/2001 completed. Loss: 0.0446\n",
      "Epoch 775/2001 completed. Loss: 0.0439\n",
      "Epoch 776/2001 completed. Loss: 0.0459\n",
      "Epoch 777/2001 completed. Loss: 0.0442\n",
      "Epoch 778/2001 completed. Loss: 0.0450\n",
      "Epoch 779/2001 completed. Loss: 0.0450\n",
      "Epoch 780/2001 completed. Loss: 0.0434\n",
      "Epoch 781/2001 completed. Loss: 0.0438\n",
      "Epoch 782/2001 completed. Loss: 0.0438\n",
      "Epoch 783/2001 completed. Loss: 0.0456\n",
      "Epoch 784/2001 completed. Loss: 0.0432\n",
      "Epoch 785/2001 completed. Loss: 0.0452\n",
      "Epoch 786/2001 completed. Loss: 0.0448\n",
      "Epoch 787/2001 completed. Loss: 0.0447\n",
      "Epoch 788/2001 completed. Loss: 0.0455\n",
      "Epoch 789/2001 completed. Loss: 0.0460\n",
      "Epoch 790/2001 completed. Loss: 0.0442\n",
      "Epoch 791/2001 completed. Loss: 0.0428\n",
      "Epoch 792/2001 completed. Loss: 0.0442\n",
      "Epoch 793/2001 completed. Loss: 0.0451\n",
      "Epoch 794/2001 completed. Loss: 0.0442\n",
      "Epoch 795/2001 completed. Loss: 0.0439\n",
      "Epoch 796/2001 completed. Loss: 0.0461\n",
      "Epoch 797/2001 completed. Loss: 0.0451\n",
      "Epoch 798/2001 completed. Loss: 0.0444\n",
      "Epoch 799/2001 completed. Loss: 0.0455\n",
      "Epoch 800/2001 completed. Loss: 0.0449\n",
      "Epoch 801/2001 completed. Loss: 0.0443\n",
      "Epoch 802/2001 completed. Loss: 0.0437\n",
      "Epoch 803/2001 completed. Loss: 0.0441\n",
      "Epoch 804/2001 completed. Loss: 0.0440\n",
      "Epoch 805/2001 completed. Loss: 0.0440\n",
      "Epoch 806/2001 completed. Loss: 0.0448\n",
      "Epoch 807/2001 completed. Loss: 0.0442\n",
      "Epoch 808/2001 completed. Loss: 0.0441\n",
      "Epoch 809/2001 completed. Loss: 0.0437\n",
      "Epoch 810/2001 completed. Loss: 0.0447\n",
      "Epoch 811/2001 completed. Loss: 0.0442\n",
      "Epoch 812/2001 completed. Loss: 0.0439\n",
      "Epoch 813/2001 completed. Loss: 0.0437\n",
      "Epoch 814/2001 completed. Loss: 0.0429\n",
      "Epoch 815/2001 completed. Loss: 0.0448\n",
      "Epoch 816/2001 completed. Loss: 0.0440\n",
      "Epoch 817/2001 completed. Loss: 0.0439\n",
      "Epoch 818/2001 completed. Loss: 0.0452\n",
      "Epoch 819/2001 completed. Loss: 0.0468\n",
      "Epoch 820/2001 completed. Loss: 0.0432\n",
      "Epoch 821/2001 completed. Loss: 0.0437\n",
      "Epoch 822/2001 completed. Loss: 0.0449\n",
      "Epoch 823/2001 completed. Loss: 0.0444\n",
      "Epoch 824/2001 completed. Loss: 0.0439\n",
      "Epoch 825/2001 completed. Loss: 0.0434\n",
      "Epoch 826/2001 completed. Loss: 0.0430\n",
      "Epoch 827/2001 completed. Loss: 0.0440\n",
      "Epoch 828/2001 completed. Loss: 0.0452\n",
      "Epoch 829/2001 completed. Loss: 0.0437\n",
      "Epoch 830/2001 completed. Loss: 0.0451\n",
      "Epoch 831/2001 completed. Loss: 0.0428\n",
      "Epoch 832/2001 completed. Loss: 0.0441\n",
      "Epoch 833/2001 completed. Loss: 0.0437\n",
      "Epoch 834/2001 completed. Loss: 0.0446\n",
      "Epoch 835/2001 completed. Loss: 0.0451\n",
      "Epoch 836/2001 completed. Loss: 0.0427\n",
      "Epoch 837/2001 completed. Loss: 0.0454\n",
      "Epoch 838/2001 completed. Loss: 0.0426\n",
      "Epoch 839/2001 completed. Loss: 0.0451\n",
      "Epoch 840/2001 completed. Loss: 0.0443\n",
      "Epoch 841/2001 completed. Loss: 0.0445\n",
      "Epoch 842/2001 completed. Loss: 0.0437\n",
      "Epoch 843/2001 completed. Loss: 0.0427\n",
      "Epoch 844/2001 completed. Loss: 0.0437\n",
      "Epoch 845/2001 completed. Loss: 0.0422\n",
      "Epoch 846/2001 completed. Loss: 0.0445\n",
      "Epoch 847/2001 completed. Loss: 0.0440\n",
      "Epoch 848/2001 completed. Loss: 0.0453\n",
      "Epoch 849/2001 completed. Loss: 0.0426\n",
      "Epoch 850/2001 completed. Loss: 0.0424\n",
      "Epoch 851/2001 completed. Loss: 0.0438\n",
      "Epoch 852/2001 completed. Loss: 0.0441\n",
      "Epoch 853/2001 completed. Loss: 0.0441\n",
      "Epoch 854/2001 completed. Loss: 0.0449\n",
      "Epoch 855/2001 completed. Loss: 0.0441\n",
      "Epoch 856/2001 completed. Loss: 0.0450\n",
      "Epoch 857/2001 completed. Loss: 0.0444\n",
      "Epoch 858/2001 completed. Loss: 0.0432\n",
      "Epoch 859/2001 completed. Loss: 0.0447\n",
      "Epoch 860/2001 completed. Loss: 0.0429\n",
      "Epoch 861/2001 completed. Loss: 0.0454\n",
      "Epoch 862/2001 completed. Loss: 0.0439\n",
      "Epoch 863/2001 completed. Loss: 0.0449\n",
      "Epoch 864/2001 completed. Loss: 0.0442\n",
      "Epoch 865/2001 completed. Loss: 0.0436\n",
      "Epoch 866/2001 completed. Loss: 0.0438\n",
      "Epoch 867/2001 completed. Loss: 0.0433\n",
      "Epoch 868/2001 completed. Loss: 0.0440\n",
      "Epoch 869/2001 completed. Loss: 0.0431\n",
      "Epoch 870/2001 completed. Loss: 0.0440\n",
      "Epoch 871/2001 completed. Loss: 0.0426\n",
      "Epoch 872/2001 completed. Loss: 0.0422\n",
      "Epoch 873/2001 completed. Loss: 0.0412\n",
      "Epoch 874/2001 completed. Loss: 0.0440\n",
      "Epoch 875/2001 completed. Loss: 0.0416\n",
      "Epoch 876/2001 completed. Loss: 0.0436\n",
      "Epoch 877/2001 completed. Loss: 0.0446\n",
      "Epoch 878/2001 completed. Loss: 0.0448\n",
      "Epoch 879/2001 completed. Loss: 0.0453\n",
      "Epoch 880/2001 completed. Loss: 0.0433\n",
      "Epoch 881/2001 completed. Loss: 0.0436\n",
      "Epoch 882/2001 completed. Loss: 0.0455\n",
      "Epoch 883/2001 completed. Loss: 0.0442\n",
      "Epoch 884/2001 completed. Loss: 0.0412\n",
      "Epoch 885/2001 completed. Loss: 0.0440\n",
      "Epoch 886/2001 completed. Loss: 0.0443\n",
      "Epoch 887/2001 completed. Loss: 0.0412\n",
      "Epoch 888/2001 completed. Loss: 0.0455\n",
      "Epoch 889/2001 completed. Loss: 0.0441\n",
      "Epoch 890/2001 completed. Loss: 0.0433\n",
      "Epoch 891/2001 completed. Loss: 0.0414\n",
      "Epoch 892/2001 completed. Loss: 0.0415\n",
      "Epoch 893/2001 completed. Loss: 0.0447\n",
      "Epoch 894/2001 completed. Loss: 0.0433\n",
      "Epoch 895/2001 completed. Loss: 0.0425\n",
      "Epoch 896/2001 completed. Loss: 0.0416\n",
      "Epoch 897/2001 completed. Loss: 0.0418\n",
      "Epoch 898/2001 completed. Loss: 0.0429\n",
      "Epoch 899/2001 completed. Loss: 0.0443\n",
      "Epoch 900/2001 completed. Loss: 0.0439\n",
      "Epoch 901/2001 completed. Loss: 0.0424\n",
      "Epoch 902/2001 completed. Loss: 0.0422\n",
      "Epoch 903/2001 completed. Loss: 0.0429\n",
      "Epoch 904/2001 completed. Loss: 0.0448\n",
      "Epoch 905/2001 completed. Loss: 0.0441\n",
      "Epoch 906/2001 completed. Loss: 0.0456\n",
      "Epoch 907/2001 completed. Loss: 0.0439\n",
      "Epoch 908/2001 completed. Loss: 0.0423\n",
      "Epoch 909/2001 completed. Loss: 0.0428\n",
      "Epoch 910/2001 completed. Loss: 0.0411\n",
      "Epoch 911/2001 completed. Loss: 0.0430\n",
      "Epoch 912/2001 completed. Loss: 0.0415\n",
      "Epoch 913/2001 completed. Loss: 0.0419\n",
      "Epoch 914/2001 completed. Loss: 0.0419\n",
      "Epoch 915/2001 completed. Loss: 0.0432\n",
      "Epoch 916/2001 completed. Loss: 0.0433\n",
      "Epoch 917/2001 completed. Loss: 0.0447\n",
      "Epoch 918/2001 completed. Loss: 0.0424\n",
      "Epoch 919/2001 completed. Loss: 0.0442\n",
      "Epoch 920/2001 completed. Loss: 0.0427\n",
      "Epoch 921/2001 completed. Loss: 0.0430\n",
      "Epoch 922/2001 completed. Loss: 0.0436\n",
      "Epoch 923/2001 completed. Loss: 0.0440\n",
      "Epoch 924/2001 completed. Loss: 0.0422\n",
      "Epoch 925/2001 completed. Loss: 0.0430\n",
      "Epoch 926/2001 completed. Loss: 0.0411\n",
      "Epoch 927/2001 completed. Loss: 0.0433\n",
      "Epoch 928/2001 completed. Loss: 0.0434\n",
      "Epoch 929/2001 completed. Loss: 0.0411\n",
      "Epoch 930/2001 completed. Loss: 0.0422\n",
      "Epoch 931/2001 completed. Loss: 0.0426\n",
      "Epoch 932/2001 completed. Loss: 0.0435\n",
      "Epoch 933/2001 completed. Loss: 0.0453\n",
      "Epoch 934/2001 completed. Loss: 0.0440\n",
      "Epoch 935/2001 completed. Loss: 0.0439\n",
      "Epoch 936/2001 completed. Loss: 0.0426\n",
      "Epoch 937/2001 completed. Loss: 0.0433\n",
      "Epoch 938/2001 completed. Loss: 0.0431\n",
      "Epoch 939/2001 completed. Loss: 0.0441\n",
      "Epoch 940/2001 completed. Loss: 0.0448\n",
      "Epoch 941/2001 completed. Loss: 0.0421\n",
      "Epoch 942/2001 completed. Loss: 0.0436\n",
      "Epoch 943/2001 completed. Loss: 0.0420\n",
      "Epoch 944/2001 completed. Loss: 0.0420\n",
      "Epoch 945/2001 completed. Loss: 0.0429\n",
      "Epoch 946/2001 completed. Loss: 0.0434\n",
      "Epoch 947/2001 completed. Loss: 0.0444\n",
      "Epoch 948/2001 completed. Loss: 0.0437\n",
      "Epoch 949/2001 completed. Loss: 0.0440\n",
      "Epoch 950/2001 completed. Loss: 0.0432\n",
      "Epoch 951/2001 completed. Loss: 0.0420\n",
      "Epoch 952/2001 completed. Loss: 0.0425\n",
      "Epoch 953/2001 completed. Loss: 0.0432\n",
      "Epoch 954/2001 completed. Loss: 0.0428\n",
      "Epoch 955/2001 completed. Loss: 0.0442\n",
      "Epoch 956/2001 completed. Loss: 0.0411\n",
      "Epoch 957/2001 completed. Loss: 0.0428\n",
      "Epoch 958/2001 completed. Loss: 0.0448\n",
      "Epoch 959/2001 completed. Loss: 0.0427\n",
      "Epoch 960/2001 completed. Loss: 0.0427\n",
      "Epoch 961/2001 completed. Loss: 0.0426\n",
      "Epoch 962/2001 completed. Loss: 0.0413\n",
      "Epoch 963/2001 completed. Loss: 0.0421\n",
      "Epoch 964/2001 completed. Loss: 0.0444\n",
      "Epoch 965/2001 completed. Loss: 0.0434\n",
      "Epoch 966/2001 completed. Loss: 0.0412\n",
      "Epoch 967/2001 completed. Loss: 0.0427\n",
      "Epoch 968/2001 completed. Loss: 0.0399\n",
      "Epoch 969/2001 completed. Loss: 0.0437\n",
      "Epoch 970/2001 completed. Loss: 0.0433\n",
      "Epoch 971/2001 completed. Loss: 0.0422\n",
      "Epoch 972/2001 completed. Loss: 0.0425\n",
      "Epoch 973/2001 completed. Loss: 0.0433\n",
      "Epoch 974/2001 completed. Loss: 0.0411\n",
      "Epoch 975/2001 completed. Loss: 0.0440\n",
      "Epoch 976/2001 completed. Loss: 0.0416\n",
      "Epoch 977/2001 completed. Loss: 0.0422\n",
      "Epoch 978/2001 completed. Loss: 0.0421\n",
      "Epoch 979/2001 completed. Loss: 0.0423\n",
      "Epoch 980/2001 completed. Loss: 0.0428\n",
      "Epoch 981/2001 completed. Loss: 0.0436\n",
      "Epoch 982/2001 completed. Loss: 0.0423\n",
      "Epoch 983/2001 completed. Loss: 0.0448\n",
      "Epoch 984/2001 completed. Loss: 0.0429\n",
      "Epoch 985/2001 completed. Loss: 0.0412\n",
      "Epoch 986/2001 completed. Loss: 0.0421\n",
      "Epoch 987/2001 completed. Loss: 0.0414\n",
      "Epoch 988/2001 completed. Loss: 0.0416\n",
      "Epoch 989/2001 completed. Loss: 0.0434\n",
      "Epoch 990/2001 completed. Loss: 0.0434\n",
      "Epoch 991/2001 completed. Loss: 0.0414\n",
      "Epoch 992/2001 completed. Loss: 0.0409\n",
      "Epoch 993/2001 completed. Loss: 0.0431\n",
      "Epoch 994/2001 completed. Loss: 0.0429\n",
      "Epoch 995/2001 completed. Loss: 0.0420\n",
      "Epoch 996/2001 completed. Loss: 0.0422\n",
      "Epoch 997/2001 completed. Loss: 0.0415\n",
      "Epoch 998/2001 completed. Loss: 0.0404\n",
      "Epoch 999/2001 completed. Loss: 0.0425\n",
      "Epoch 1000/2001 completed. Loss: 0.0435\n",
      "Epoch 1001/2001 completed. Loss: 0.0411\n",
      "Epoch 1002/2001 completed. Loss: 0.0441\n",
      "Epoch 1003/2001 completed. Loss: 0.0414\n",
      "Epoch 1004/2001 completed. Loss: 0.0430\n",
      "Epoch 1005/2001 completed. Loss: 0.0450\n",
      "Epoch 1006/2001 completed. Loss: 0.0408\n",
      "Epoch 1007/2001 completed. Loss: 0.0415\n",
      "Epoch 1008/2001 completed. Loss: 0.0438\n",
      "Epoch 1009/2001 completed. Loss: 0.0415\n",
      "Epoch 1010/2001 completed. Loss: 0.0418\n",
      "Epoch 1011/2001 completed. Loss: 0.0429\n",
      "Epoch 1012/2001 completed. Loss: 0.0407\n",
      "Epoch 1013/2001 completed. Loss: 0.0400\n",
      "Epoch 1014/2001 completed. Loss: 0.0432\n",
      "Epoch 1015/2001 completed. Loss: 0.0401\n",
      "Epoch 1016/2001 completed. Loss: 0.0410\n",
      "Epoch 1017/2001 completed. Loss: 0.0428\n",
      "Epoch 1018/2001 completed. Loss: 0.0435\n",
      "Epoch 1019/2001 completed. Loss: 0.0411\n",
      "Epoch 1020/2001 completed. Loss: 0.0415\n",
      "Epoch 1021/2001 completed. Loss: 0.0410\n",
      "Epoch 1022/2001 completed. Loss: 0.0430\n",
      "Epoch 1023/2001 completed. Loss: 0.0414\n",
      "Epoch 1024/2001 completed. Loss: 0.0413\n",
      "Epoch 1025/2001 completed. Loss: 0.0421\n",
      "Epoch 1026/2001 completed. Loss: 0.0425\n",
      "Epoch 1027/2001 completed. Loss: 0.0436\n",
      "Epoch 1028/2001 completed. Loss: 0.0430\n",
      "Epoch 1029/2001 completed. Loss: 0.0431\n",
      "Epoch 1030/2001 completed. Loss: 0.0437\n",
      "Epoch 1031/2001 completed. Loss: 0.0433\n",
      "Epoch 1032/2001 completed. Loss: 0.0414\n",
      "Epoch 1033/2001 completed. Loss: 0.0410\n",
      "Epoch 1034/2001 completed. Loss: 0.0438\n",
      "Epoch 1035/2001 completed. Loss: 0.0393\n",
      "Epoch 1036/2001 completed. Loss: 0.0428\n",
      "Epoch 1037/2001 completed. Loss: 0.0418\n",
      "Epoch 1038/2001 completed. Loss: 0.0426\n",
      "Epoch 1039/2001 completed. Loss: 0.0413\n",
      "Epoch 1040/2001 completed. Loss: 0.0423\n",
      "Epoch 1041/2001 completed. Loss: 0.0412\n",
      "Epoch 1042/2001 completed. Loss: 0.0405\n",
      "Epoch 1043/2001 completed. Loss: 0.0423\n",
      "Epoch 1044/2001 completed. Loss: 0.0429\n",
      "Epoch 1045/2001 completed. Loss: 0.0419\n",
      "Epoch 1046/2001 completed. Loss: 0.0399\n",
      "Epoch 1047/2001 completed. Loss: 0.0422\n",
      "Epoch 1048/2001 completed. Loss: 0.0406\n",
      "Epoch 1049/2001 completed. Loss: 0.0442\n",
      "Epoch 1050/2001 completed. Loss: 0.0412\n",
      "Epoch 1051/2001 completed. Loss: 0.0416\n",
      "Epoch 1052/2001 completed. Loss: 0.0408\n",
      "Epoch 1053/2001 completed. Loss: 0.0445\n",
      "Epoch 1054/2001 completed. Loss: 0.0405\n",
      "Epoch 1055/2001 completed. Loss: 0.0412\n",
      "Epoch 1056/2001 completed. Loss: 0.0415\n",
      "Epoch 1057/2001 completed. Loss: 0.0437\n",
      "Epoch 1058/2001 completed. Loss: 0.0426\n",
      "Epoch 1059/2001 completed. Loss: 0.0402\n",
      "Epoch 1060/2001 completed. Loss: 0.0406\n",
      "Epoch 1061/2001 completed. Loss: 0.0412\n",
      "Epoch 1062/2001 completed. Loss: 0.0432\n",
      "Epoch 1063/2001 completed. Loss: 0.0415\n",
      "Epoch 1064/2001 completed. Loss: 0.0428\n",
      "Epoch 1065/2001 completed. Loss: 0.0433\n",
      "Epoch 1066/2001 completed. Loss: 0.0413\n",
      "Epoch 1067/2001 completed. Loss: 0.0423\n",
      "Epoch 1068/2001 completed. Loss: 0.0431\n",
      "Epoch 1069/2001 completed. Loss: 0.0414\n",
      "Epoch 1070/2001 completed. Loss: 0.0397\n",
      "Epoch 1071/2001 completed. Loss: 0.0425\n",
      "Epoch 1072/2001 completed. Loss: 0.0402\n",
      "Epoch 1073/2001 completed. Loss: 0.0406\n",
      "Epoch 1074/2001 completed. Loss: 0.0427\n",
      "Epoch 1075/2001 completed. Loss: 0.0418\n",
      "Epoch 1076/2001 completed. Loss: 0.0423\n",
      "Epoch 1077/2001 completed. Loss: 0.0413\n",
      "Epoch 1078/2001 completed. Loss: 0.0410\n",
      "Epoch 1079/2001 completed. Loss: 0.0409\n",
      "Epoch 1080/2001 completed. Loss: 0.0439\n",
      "Epoch 1081/2001 completed. Loss: 0.0421\n",
      "Epoch 1082/2001 completed. Loss: 0.0430\n",
      "Epoch 1083/2001 completed. Loss: 0.0412\n",
      "Epoch 1084/2001 completed. Loss: 0.0435\n",
      "Epoch 1085/2001 completed. Loss: 0.0405\n",
      "Epoch 1086/2001 completed. Loss: 0.0437\n",
      "Epoch 1087/2001 completed. Loss: 0.0402\n",
      "Epoch 1088/2001 completed. Loss: 0.0419\n",
      "Epoch 1089/2001 completed. Loss: 0.0412\n",
      "Epoch 1090/2001 completed. Loss: 0.0407\n",
      "Epoch 1091/2001 completed. Loss: 0.0394\n",
      "Epoch 1092/2001 completed. Loss: 0.0415\n",
      "Epoch 1093/2001 completed. Loss: 0.0421\n",
      "Epoch 1094/2001 completed. Loss: 0.0425\n",
      "Epoch 1095/2001 completed. Loss: 0.0429\n",
      "Epoch 1096/2001 completed. Loss: 0.0438\n",
      "Epoch 1097/2001 completed. Loss: 0.0425\n",
      "Epoch 1098/2001 completed. Loss: 0.0403\n",
      "Epoch 1099/2001 completed. Loss: 0.0424\n",
      "Epoch 1100/2001 completed. Loss: 0.0418\n",
      "Epoch 1101/2001 completed. Loss: 0.0419\n",
      "Epoch 1102/2001 completed. Loss: 0.0428\n",
      "Epoch 1103/2001 completed. Loss: 0.0423\n",
      "Epoch 1104/2001 completed. Loss: 0.0434\n",
      "Epoch 1105/2001 completed. Loss: 0.0419\n",
      "Epoch 1106/2001 completed. Loss: 0.0420\n",
      "Epoch 1107/2001 completed. Loss: 0.0431\n",
      "Epoch 1108/2001 completed. Loss: 0.0404\n",
      "Epoch 1109/2001 completed. Loss: 0.0401\n",
      "Epoch 1110/2001 completed. Loss: 0.0382\n",
      "Epoch 1111/2001 completed. Loss: 0.0397\n",
      "Epoch 1112/2001 completed. Loss: 0.0403\n",
      "Epoch 1113/2001 completed. Loss: 0.0421\n",
      "Epoch 1114/2001 completed. Loss: 0.0410\n",
      "Epoch 1115/2001 completed. Loss: 0.0393\n",
      "Epoch 1116/2001 completed. Loss: 0.0426\n",
      "Epoch 1117/2001 completed. Loss: 0.0414\n",
      "Epoch 1118/2001 completed. Loss: 0.0437\n",
      "Epoch 1119/2001 completed. Loss: 0.0410\n",
      "Epoch 1120/2001 completed. Loss: 0.0416\n",
      "Epoch 1121/2001 completed. Loss: 0.0393\n",
      "Epoch 1122/2001 completed. Loss: 0.0390\n",
      "Epoch 1123/2001 completed. Loss: 0.0407\n",
      "Epoch 1124/2001 completed. Loss: 0.0408\n",
      "Epoch 1125/2001 completed. Loss: 0.0409\n",
      "Epoch 1126/2001 completed. Loss: 0.0414\n",
      "Epoch 1127/2001 completed. Loss: 0.0419\n",
      "Epoch 1128/2001 completed. Loss: 0.0414\n",
      "Epoch 1129/2001 completed. Loss: 0.0402\n",
      "Epoch 1130/2001 completed. Loss: 0.0403\n",
      "Epoch 1131/2001 completed. Loss: 0.0420\n",
      "Epoch 1132/2001 completed. Loss: 0.0425\n",
      "Epoch 1133/2001 completed. Loss: 0.0413\n",
      "Epoch 1134/2001 completed. Loss: 0.0418\n",
      "Epoch 1135/2001 completed. Loss: 0.0408\n",
      "Epoch 1136/2001 completed. Loss: 0.0386\n",
      "Epoch 1137/2001 completed. Loss: 0.0407\n",
      "Epoch 1138/2001 completed. Loss: 0.0427\n",
      "Epoch 1139/2001 completed. Loss: 0.0430\n",
      "Epoch 1140/2001 completed. Loss: 0.0394\n",
      "Epoch 1141/2001 completed. Loss: 0.0383\n",
      "Epoch 1142/2001 completed. Loss: 0.0418\n",
      "Epoch 1143/2001 completed. Loss: 0.0417\n",
      "Epoch 1144/2001 completed. Loss: 0.0427\n",
      "Epoch 1145/2001 completed. Loss: 0.0426\n",
      "Epoch 1146/2001 completed. Loss: 0.0404\n",
      "Epoch 1147/2001 completed. Loss: 0.0387\n",
      "Epoch 1148/2001 completed. Loss: 0.0399\n",
      "Epoch 1149/2001 completed. Loss: 0.0395\n",
      "Epoch 1150/2001 completed. Loss: 0.0403\n",
      "Epoch 1151/2001 completed. Loss: 0.0405\n",
      "Epoch 1152/2001 completed. Loss: 0.0407\n",
      "Epoch 1153/2001 completed. Loss: 0.0404\n",
      "Epoch 1154/2001 completed. Loss: 0.0396\n",
      "Epoch 1155/2001 completed. Loss: 0.0397\n",
      "Epoch 1156/2001 completed. Loss: 0.0393\n",
      "Epoch 1157/2001 completed. Loss: 0.0406\n",
      "Epoch 1158/2001 completed. Loss: 0.0404\n",
      "Epoch 1159/2001 completed. Loss: 0.0400\n",
      "Epoch 1160/2001 completed. Loss: 0.0384\n",
      "Epoch 1161/2001 completed. Loss: 0.0404\n",
      "Epoch 1162/2001 completed. Loss: 0.0412\n",
      "Epoch 1163/2001 completed. Loss: 0.0402\n",
      "Epoch 1164/2001 completed. Loss: 0.0400\n",
      "Epoch 1165/2001 completed. Loss: 0.0426\n",
      "Epoch 1166/2001 completed. Loss: 0.0399\n",
      "Epoch 1167/2001 completed. Loss: 0.0393\n",
      "Epoch 1168/2001 completed. Loss: 0.0422\n",
      "Epoch 1169/2001 completed. Loss: 0.0398\n",
      "Epoch 1170/2001 completed. Loss: 0.0391\n",
      "Epoch 1171/2001 completed. Loss: 0.0426\n",
      "Epoch 1172/2001 completed. Loss: 0.0402\n",
      "Epoch 1173/2001 completed. Loss: 0.0421\n",
      "Epoch 1174/2001 completed. Loss: 0.0408\n",
      "Epoch 1175/2001 completed. Loss: 0.0412\n",
      "Epoch 1176/2001 completed. Loss: 0.0409\n",
      "Epoch 1177/2001 completed. Loss: 0.0425\n",
      "Epoch 1178/2001 completed. Loss: 0.0398\n",
      "Epoch 1179/2001 completed. Loss: 0.0410\n",
      "Epoch 1180/2001 completed. Loss: 0.0397\n",
      "Epoch 1181/2001 completed. Loss: 0.0403\n",
      "Epoch 1182/2001 completed. Loss: 0.0414\n",
      "Epoch 1183/2001 completed. Loss: 0.0406\n",
      "Epoch 1184/2001 completed. Loss: 0.0399\n",
      "Epoch 1185/2001 completed. Loss: 0.0389\n",
      "Epoch 1186/2001 completed. Loss: 0.0399\n",
      "Epoch 1187/2001 completed. Loss: 0.0385\n",
      "Epoch 1188/2001 completed. Loss: 0.0402\n",
      "Epoch 1189/2001 completed. Loss: 0.0394\n",
      "Epoch 1190/2001 completed. Loss: 0.0397\n",
      "Epoch 1191/2001 completed. Loss: 0.0421\n",
      "Epoch 1192/2001 completed. Loss: 0.0398\n",
      "Epoch 1193/2001 completed. Loss: 0.0409\n",
      "Epoch 1194/2001 completed. Loss: 0.0421\n",
      "Epoch 1195/2001 completed. Loss: 0.0411\n",
      "Epoch 1196/2001 completed. Loss: 0.0399\n",
      "Epoch 1197/2001 completed. Loss: 0.0410\n",
      "Epoch 1198/2001 completed. Loss: 0.0413\n",
      "Epoch 1199/2001 completed. Loss: 0.0407\n",
      "Epoch 1200/2001 completed. Loss: 0.0405\n",
      "Epoch 1201/2001 completed. Loss: 0.0398\n",
      "Epoch 1202/2001 completed. Loss: 0.0407\n",
      "Epoch 1203/2001 completed. Loss: 0.0406\n",
      "Epoch 1204/2001 completed. Loss: 0.0418\n",
      "Epoch 1205/2001 completed. Loss: 0.0384\n",
      "Epoch 1206/2001 completed. Loss: 0.0395\n",
      "Epoch 1207/2001 completed. Loss: 0.0419\n",
      "Epoch 1208/2001 completed. Loss: 0.0401\n",
      "Epoch 1209/2001 completed. Loss: 0.0401\n",
      "Epoch 1210/2001 completed. Loss: 0.0388\n",
      "Epoch 1211/2001 completed. Loss: 0.0405\n",
      "Epoch 1212/2001 completed. Loss: 0.0403\n",
      "Epoch 1213/2001 completed. Loss: 0.0410\n",
      "Epoch 1214/2001 completed. Loss: 0.0410\n",
      "Epoch 1215/2001 completed. Loss: 0.0414\n",
      "Epoch 1216/2001 completed. Loss: 0.0407\n",
      "Epoch 1217/2001 completed. Loss: 0.0402\n",
      "Epoch 1218/2001 completed. Loss: 0.0384\n",
      "Epoch 1219/2001 completed. Loss: 0.0406\n",
      "Epoch 1220/2001 completed. Loss: 0.0400\n",
      "Epoch 1221/2001 completed. Loss: 0.0403\n",
      "Epoch 1222/2001 completed. Loss: 0.0415\n",
      "Epoch 1223/2001 completed. Loss: 0.0415\n",
      "Epoch 1224/2001 completed. Loss: 0.0393\n",
      "Epoch 1225/2001 completed. Loss: 0.0400\n",
      "Epoch 1226/2001 completed. Loss: 0.0413\n",
      "Epoch 1227/2001 completed. Loss: 0.0405\n",
      "Epoch 1228/2001 completed. Loss: 0.0384\n",
      "Epoch 1229/2001 completed. Loss: 0.0394\n",
      "Epoch 1230/2001 completed. Loss: 0.0414\n",
      "Epoch 1231/2001 completed. Loss: 0.0409\n",
      "Epoch 1232/2001 completed. Loss: 0.0409\n",
      "Epoch 1233/2001 completed. Loss: 0.0413\n",
      "Epoch 1234/2001 completed. Loss: 0.0412\n",
      "Epoch 1235/2001 completed. Loss: 0.0398\n",
      "Epoch 1236/2001 completed. Loss: 0.0410\n",
      "Epoch 1237/2001 completed. Loss: 0.0409\n",
      "Epoch 1238/2001 completed. Loss: 0.0418\n",
      "Epoch 1239/2001 completed. Loss: 0.0407\n",
      "Epoch 1240/2001 completed. Loss: 0.0393\n",
      "Epoch 1241/2001 completed. Loss: 0.0400\n",
      "Epoch 1242/2001 completed. Loss: 0.0429\n",
      "Epoch 1243/2001 completed. Loss: 0.0411\n",
      "Epoch 1244/2001 completed. Loss: 0.0390\n",
      "Epoch 1245/2001 completed. Loss: 0.0388\n",
      "Epoch 1246/2001 completed. Loss: 0.0383\n",
      "Epoch 1247/2001 completed. Loss: 0.0420\n",
      "Epoch 1248/2001 completed. Loss: 0.0400\n",
      "Epoch 1249/2001 completed. Loss: 0.0410\n",
      "Epoch 1250/2001 completed. Loss: 0.0389\n",
      "Epoch 1251/2001 completed. Loss: 0.0416\n",
      "Epoch 1252/2001 completed. Loss: 0.0404\n",
      "Epoch 1253/2001 completed. Loss: 0.0401\n",
      "Epoch 1254/2001 completed. Loss: 0.0417\n",
      "Epoch 1255/2001 completed. Loss: 0.0388\n",
      "Epoch 1256/2001 completed. Loss: 0.0412\n",
      "Epoch 1257/2001 completed. Loss: 0.0404\n",
      "Epoch 1258/2001 completed. Loss: 0.0416\n",
      "Epoch 1259/2001 completed. Loss: 0.0410\n",
      "Epoch 1260/2001 completed. Loss: 0.0402\n",
      "Epoch 1261/2001 completed. Loss: 0.0397\n",
      "Epoch 1262/2001 completed. Loss: 0.0401\n",
      "Epoch 1263/2001 completed. Loss: 0.0395\n",
      "Epoch 1264/2001 completed. Loss: 0.0402\n",
      "Epoch 1265/2001 completed. Loss: 0.0394\n",
      "Epoch 1266/2001 completed. Loss: 0.0409\n",
      "Epoch 1267/2001 completed. Loss: 0.0395\n",
      "Epoch 1268/2001 completed. Loss: 0.0398\n",
      "Epoch 1269/2001 completed. Loss: 0.0397\n",
      "Epoch 1270/2001 completed. Loss: 0.0408\n",
      "Epoch 1271/2001 completed. Loss: 0.0394\n",
      "Epoch 1272/2001 completed. Loss: 0.0412\n",
      "Epoch 1273/2001 completed. Loss: 0.0406\n",
      "Epoch 1274/2001 completed. Loss: 0.0414\n",
      "Epoch 1275/2001 completed. Loss: 0.0388\n",
      "Epoch 1276/2001 completed. Loss: 0.0399\n",
      "Epoch 1277/2001 completed. Loss: 0.0407\n",
      "Epoch 1278/2001 completed. Loss: 0.0392\n",
      "Epoch 1279/2001 completed. Loss: 0.0390\n",
      "Epoch 1280/2001 completed. Loss: 0.0386\n",
      "Epoch 1281/2001 completed. Loss: 0.0398\n",
      "Epoch 1282/2001 completed. Loss: 0.0402\n",
      "Epoch 1283/2001 completed. Loss: 0.0408\n",
      "Epoch 1284/2001 completed. Loss: 0.0405\n",
      "Epoch 1285/2001 completed. Loss: 0.0383\n",
      "Epoch 1286/2001 completed. Loss: 0.0381\n",
      "Epoch 1287/2001 completed. Loss: 0.0409\n",
      "Epoch 1288/2001 completed. Loss: 0.0416\n",
      "Epoch 1289/2001 completed. Loss: 0.0387\n",
      "Epoch 1290/2001 completed. Loss: 0.0405\n",
      "Epoch 1291/2001 completed. Loss: 0.0393\n",
      "Epoch 1292/2001 completed. Loss: 0.0401\n",
      "Epoch 1293/2001 completed. Loss: 0.0414\n",
      "Epoch 1294/2001 completed. Loss: 0.0381\n",
      "Epoch 1295/2001 completed. Loss: 0.0390\n",
      "Epoch 1296/2001 completed. Loss: 0.0401\n",
      "Epoch 1297/2001 completed. Loss: 0.0414\n",
      "Epoch 1298/2001 completed. Loss: 0.0396\n",
      "Epoch 1299/2001 completed. Loss: 0.0400\n",
      "Epoch 1300/2001 completed. Loss: 0.0405\n",
      "Epoch 1301/2001 completed. Loss: 0.0409\n",
      "Epoch 1302/2001 completed. Loss: 0.0400\n",
      "Epoch 1303/2001 completed. Loss: 0.0412\n",
      "Epoch 1304/2001 completed. Loss: 0.0393\n",
      "Epoch 1305/2001 completed. Loss: 0.0406\n",
      "Epoch 1306/2001 completed. Loss: 0.0401\n",
      "Epoch 1307/2001 completed. Loss: 0.0385\n",
      "Epoch 1308/2001 completed. Loss: 0.0396\n",
      "Epoch 1309/2001 completed. Loss: 0.0406\n",
      "Epoch 1310/2001 completed. Loss: 0.0399\n",
      "Epoch 1311/2001 completed. Loss: 0.0390\n",
      "Epoch 1312/2001 completed. Loss: 0.0408\n",
      "Epoch 1313/2001 completed. Loss: 0.0405\n",
      "Epoch 1314/2001 completed. Loss: 0.0393\n",
      "Epoch 1315/2001 completed. Loss: 0.0395\n",
      "Epoch 1316/2001 completed. Loss: 0.0408\n",
      "Epoch 1317/2001 completed. Loss: 0.0403\n",
      "Epoch 1318/2001 completed. Loss: 0.0373\n",
      "Epoch 1319/2001 completed. Loss: 0.0396\n",
      "Epoch 1320/2001 completed. Loss: 0.0410\n",
      "Epoch 1321/2001 completed. Loss: 0.0410\n",
      "Epoch 1322/2001 completed. Loss: 0.0378\n",
      "Epoch 1323/2001 completed. Loss: 0.0401\n",
      "Epoch 1324/2001 completed. Loss: 0.0398\n",
      "Epoch 1325/2001 completed. Loss: 0.0389\n",
      "Epoch 1326/2001 completed. Loss: 0.0400\n",
      "Epoch 1327/2001 completed. Loss: 0.0386\n",
      "Epoch 1328/2001 completed. Loss: 0.0388\n",
      "Epoch 1329/2001 completed. Loss: 0.0397\n",
      "Epoch 1330/2001 completed. Loss: 0.0409\n",
      "Epoch 1331/2001 completed. Loss: 0.0402\n",
      "Epoch 1332/2001 completed. Loss: 0.0388\n",
      "Epoch 1333/2001 completed. Loss: 0.0397\n",
      "Epoch 1334/2001 completed. Loss: 0.0401\n",
      "Epoch 1335/2001 completed. Loss: 0.0392\n",
      "Epoch 1336/2001 completed. Loss: 0.0392\n",
      "Epoch 1337/2001 completed. Loss: 0.0388\n",
      "Epoch 1338/2001 completed. Loss: 0.0395\n",
      "Epoch 1339/2001 completed. Loss: 0.0408\n",
      "Epoch 1340/2001 completed. Loss: 0.0386\n",
      "Epoch 1341/2001 completed. Loss: 0.0379\n",
      "Epoch 1342/2001 completed. Loss: 0.0390\n",
      "Epoch 1343/2001 completed. Loss: 0.0396\n",
      "Epoch 1344/2001 completed. Loss: 0.0395\n",
      "Epoch 1345/2001 completed. Loss: 0.0397\n",
      "Epoch 1346/2001 completed. Loss: 0.0417\n",
      "Epoch 1347/2001 completed. Loss: 0.0410\n",
      "Epoch 1348/2001 completed. Loss: 0.0411\n",
      "Epoch 1349/2001 completed. Loss: 0.0398\n",
      "Epoch 1350/2001 completed. Loss: 0.0414\n",
      "Epoch 1351/2001 completed. Loss: 0.0398\n",
      "Epoch 1352/2001 completed. Loss: 0.0389\n",
      "Epoch 1353/2001 completed. Loss: 0.0374\n",
      "Epoch 1354/2001 completed. Loss: 0.0403\n",
      "Epoch 1355/2001 completed. Loss: 0.0386\n",
      "Epoch 1356/2001 completed. Loss: 0.0394\n",
      "Epoch 1357/2001 completed. Loss: 0.0411\n",
      "Epoch 1358/2001 completed. Loss: 0.0392\n",
      "Epoch 1359/2001 completed. Loss: 0.0406\n",
      "Epoch 1360/2001 completed. Loss: 0.0386\n",
      "Epoch 1361/2001 completed. Loss: 0.0402\n",
      "Epoch 1362/2001 completed. Loss: 0.0386\n",
      "Epoch 1363/2001 completed. Loss: 0.0410\n",
      "Epoch 1364/2001 completed. Loss: 0.0392\n",
      "Epoch 1365/2001 completed. Loss: 0.0390\n",
      "Epoch 1366/2001 completed. Loss: 0.0372\n",
      "Epoch 1367/2001 completed. Loss: 0.0380\n",
      "Epoch 1368/2001 completed. Loss: 0.0394\n",
      "Epoch 1369/2001 completed. Loss: 0.0401\n",
      "Epoch 1370/2001 completed. Loss: 0.0379\n",
      "Epoch 1371/2001 completed. Loss: 0.0406\n",
      "Epoch 1372/2001 completed. Loss: 0.0403\n",
      "Epoch 1373/2001 completed. Loss: 0.0395\n",
      "Epoch 1374/2001 completed. Loss: 0.0391\n",
      "Epoch 1375/2001 completed. Loss: 0.0409\n",
      "Epoch 1376/2001 completed. Loss: 0.0404\n",
      "Epoch 1377/2001 completed. Loss: 0.0385\n",
      "Epoch 1378/2001 completed. Loss: 0.0399\n",
      "Epoch 1379/2001 completed. Loss: 0.0403\n",
      "Epoch 1380/2001 completed. Loss: 0.0392\n",
      "Epoch 1381/2001 completed. Loss: 0.0405\n",
      "Epoch 1382/2001 completed. Loss: 0.0388\n",
      "Epoch 1383/2001 completed. Loss: 0.0394\n",
      "Epoch 1384/2001 completed. Loss: 0.0379\n",
      "Epoch 1385/2001 completed. Loss: 0.0388\n",
      "Epoch 1386/2001 completed. Loss: 0.0397\n",
      "Epoch 1387/2001 completed. Loss: 0.0384\n",
      "Epoch 1388/2001 completed. Loss: 0.0402\n",
      "Epoch 1389/2001 completed. Loss: 0.0394\n",
      "Epoch 1390/2001 completed. Loss: 0.0392\n",
      "Epoch 1391/2001 completed. Loss: 0.0384\n",
      "Epoch 1392/2001 completed. Loss: 0.0400\n",
      "Epoch 1393/2001 completed. Loss: 0.0398\n",
      "Epoch 1394/2001 completed. Loss: 0.0395\n",
      "Epoch 1395/2001 completed. Loss: 0.0392\n",
      "Epoch 1396/2001 completed. Loss: 0.0378\n",
      "Epoch 1397/2001 completed. Loss: 0.0404\n",
      "Epoch 1398/2001 completed. Loss: 0.0373\n",
      "Epoch 1399/2001 completed. Loss: 0.0398\n",
      "Epoch 1400/2001 completed. Loss: 0.0379\n",
      "Epoch 1401/2001 completed. Loss: 0.0387\n",
      "Epoch 1402/2001 completed. Loss: 0.0381\n",
      "Epoch 1403/2001 completed. Loss: 0.0387\n",
      "Epoch 1404/2001 completed. Loss: 0.0385\n",
      "Epoch 1405/2001 completed. Loss: 0.0398\n",
      "Epoch 1406/2001 completed. Loss: 0.0384\n",
      "Epoch 1407/2001 completed. Loss: 0.0404\n",
      "Epoch 1408/2001 completed. Loss: 0.0374\n",
      "Epoch 1409/2001 completed. Loss: 0.0409\n",
      "Epoch 1410/2001 completed. Loss: 0.0375\n",
      "Epoch 1411/2001 completed. Loss: 0.0384\n",
      "Epoch 1412/2001 completed. Loss: 0.0383\n",
      "Epoch 1413/2001 completed. Loss: 0.0401\n",
      "Epoch 1414/2001 completed. Loss: 0.0397\n",
      "Epoch 1415/2001 completed. Loss: 0.0411\n",
      "Epoch 1416/2001 completed. Loss: 0.0392\n",
      "Epoch 1417/2001 completed. Loss: 0.0397\n",
      "Epoch 1418/2001 completed. Loss: 0.0391\n",
      "Epoch 1419/2001 completed. Loss: 0.0397\n",
      "Epoch 1420/2001 completed. Loss: 0.0402\n",
      "Epoch 1421/2001 completed. Loss: 0.0381\n",
      "Epoch 1422/2001 completed. Loss: 0.0381\n",
      "Epoch 1423/2001 completed. Loss: 0.0395\n",
      "Epoch 1424/2001 completed. Loss: 0.0384\n",
      "Epoch 1425/2001 completed. Loss: 0.0390\n",
      "Epoch 1426/2001 completed. Loss: 0.0392\n",
      "Epoch 1427/2001 completed. Loss: 0.0400\n",
      "Epoch 1428/2001 completed. Loss: 0.0381\n",
      "Epoch 1429/2001 completed. Loss: 0.0403\n",
      "Epoch 1430/2001 completed. Loss: 0.0383\n",
      "Epoch 1431/2001 completed. Loss: 0.0382\n",
      "Epoch 1432/2001 completed. Loss: 0.0400\n",
      "Epoch 1433/2001 completed. Loss: 0.0416\n",
      "Epoch 1434/2001 completed. Loss: 0.0376\n",
      "Epoch 1435/2001 completed. Loss: 0.0404\n",
      "Epoch 1436/2001 completed. Loss: 0.0401\n",
      "Epoch 1437/2001 completed. Loss: 0.0392\n",
      "Epoch 1438/2001 completed. Loss: 0.0387\n",
      "Epoch 1439/2001 completed. Loss: 0.0403\n",
      "Epoch 1440/2001 completed. Loss: 0.0397\n",
      "Epoch 1441/2001 completed. Loss: 0.0427\n",
      "Epoch 1442/2001 completed. Loss: 0.0388\n",
      "Epoch 1443/2001 completed. Loss: 0.0403\n",
      "Epoch 1444/2001 completed. Loss: 0.0419\n",
      "Epoch 1445/2001 completed. Loss: 0.0398\n",
      "Epoch 1446/2001 completed. Loss: 0.0410\n",
      "Epoch 1447/2001 completed. Loss: 0.0395\n",
      "Epoch 1448/2001 completed. Loss: 0.0412\n",
      "Epoch 1449/2001 completed. Loss: 0.0387\n",
      "Epoch 1450/2001 completed. Loss: 0.0387\n",
      "Epoch 1451/2001 completed. Loss: 0.0395\n",
      "Epoch 1452/2001 completed. Loss: 0.0397\n",
      "Epoch 1453/2001 completed. Loss: 0.0382\n",
      "Epoch 1454/2001 completed. Loss: 0.0383\n",
      "Epoch 1455/2001 completed. Loss: 0.0384\n",
      "Epoch 1456/2001 completed. Loss: 0.0400\n",
      "Epoch 1457/2001 completed. Loss: 0.0394\n",
      "Epoch 1458/2001 completed. Loss: 0.0392\n",
      "Epoch 1459/2001 completed. Loss: 0.0400\n",
      "Epoch 1460/2001 completed. Loss: 0.0400\n",
      "Epoch 1461/2001 completed. Loss: 0.0379\n",
      "Epoch 1462/2001 completed. Loss: 0.0402\n",
      "Epoch 1463/2001 completed. Loss: 0.0402\n",
      "Epoch 1464/2001 completed. Loss: 0.0374\n",
      "Epoch 1465/2001 completed. Loss: 0.0388\n",
      "Epoch 1466/2001 completed. Loss: 0.0402\n",
      "Epoch 1467/2001 completed. Loss: 0.0379\n",
      "Epoch 1468/2001 completed. Loss: 0.0375\n",
      "Epoch 1469/2001 completed. Loss: 0.0384\n",
      "Epoch 1470/2001 completed. Loss: 0.0385\n",
      "Epoch 1471/2001 completed. Loss: 0.0384\n",
      "Epoch 1472/2001 completed. Loss: 0.0374\n",
      "Epoch 1473/2001 completed. Loss: 0.0399\n",
      "Epoch 1474/2001 completed. Loss: 0.0376\n",
      "Epoch 1475/2001 completed. Loss: 0.0381\n",
      "Epoch 1476/2001 completed. Loss: 0.0408\n",
      "Epoch 1477/2001 completed. Loss: 0.0393\n",
      "Epoch 1478/2001 completed. Loss: 0.0367\n",
      "Epoch 1479/2001 completed. Loss: 0.0401\n",
      "Epoch 1480/2001 completed. Loss: 0.0395\n",
      "Epoch 1481/2001 completed. Loss: 0.0388\n",
      "Epoch 1482/2001 completed. Loss: 0.0378\n",
      "Epoch 1483/2001 completed. Loss: 0.0391\n",
      "Epoch 1484/2001 completed. Loss: 0.0388\n",
      "Epoch 1485/2001 completed. Loss: 0.0417\n",
      "Epoch 1486/2001 completed. Loss: 0.0373\n",
      "Epoch 1487/2001 completed. Loss: 0.0391\n",
      "Epoch 1488/2001 completed. Loss: 0.0377\n",
      "Epoch 1489/2001 completed. Loss: 0.0407\n",
      "Epoch 1490/2001 completed. Loss: 0.0393\n",
      "Epoch 1491/2001 completed. Loss: 0.0383\n",
      "Epoch 1492/2001 completed. Loss: 0.0395\n",
      "Epoch 1493/2001 completed. Loss: 0.0381\n",
      "Epoch 1494/2001 completed. Loss: 0.0406\n",
      "Epoch 1495/2001 completed. Loss: 0.0386\n",
      "Epoch 1496/2001 completed. Loss: 0.0357\n",
      "Epoch 1497/2001 completed. Loss: 0.0397\n",
      "Epoch 1498/2001 completed. Loss: 0.0378\n",
      "Epoch 1499/2001 completed. Loss: 0.0391\n",
      "Epoch 1500/2001 completed. Loss: 0.0385\n",
      "Epoch 1501/2001 completed. Loss: 0.0379\n",
      "Epoch 1502/2001 completed. Loss: 0.0372\n",
      "Epoch 1503/2001 completed. Loss: 0.0375\n",
      "Epoch 1504/2001 completed. Loss: 0.0380\n",
      "Epoch 1505/2001 completed. Loss: 0.0424\n",
      "Epoch 1506/2001 completed. Loss: 0.0389\n",
      "Epoch 1507/2001 completed. Loss: 0.0381\n",
      "Epoch 1508/2001 completed. Loss: 0.0399\n",
      "Epoch 1509/2001 completed. Loss: 0.0383\n",
      "Epoch 1510/2001 completed. Loss: 0.0405\n",
      "Epoch 1511/2001 completed. Loss: 0.0395\n",
      "Epoch 1512/2001 completed. Loss: 0.0381\n",
      "Epoch 1513/2001 completed. Loss: 0.0381\n",
      "Epoch 1514/2001 completed. Loss: 0.0384\n",
      "Epoch 1515/2001 completed. Loss: 0.0382\n",
      "Epoch 1516/2001 completed. Loss: 0.0409\n",
      "Epoch 1517/2001 completed. Loss: 0.0398\n",
      "Epoch 1518/2001 completed. Loss: 0.0417\n",
      "Epoch 1519/2001 completed. Loss: 0.0385\n",
      "Epoch 1520/2001 completed. Loss: 0.0403\n",
      "Epoch 1521/2001 completed. Loss: 0.0379\n",
      "Epoch 1522/2001 completed. Loss: 0.0374\n",
      "Epoch 1523/2001 completed. Loss: 0.0394\n",
      "Epoch 1524/2001 completed. Loss: 0.0386\n",
      "Epoch 1525/2001 completed. Loss: 0.0405\n",
      "Epoch 1526/2001 completed. Loss: 0.0389\n",
      "Epoch 1527/2001 completed. Loss: 0.0383\n",
      "Epoch 1528/2001 completed. Loss: 0.0388\n",
      "Epoch 1529/2001 completed. Loss: 0.0370\n",
      "Epoch 1530/2001 completed. Loss: 0.0397\n",
      "Epoch 1531/2001 completed. Loss: 0.0384\n",
      "Epoch 1532/2001 completed. Loss: 0.0359\n",
      "Epoch 1533/2001 completed. Loss: 0.0368\n",
      "Epoch 1534/2001 completed. Loss: 0.0379\n",
      "Epoch 1535/2001 completed. Loss: 0.0404\n",
      "Epoch 1536/2001 completed. Loss: 0.0396\n",
      "Epoch 1537/2001 completed. Loss: 0.0388\n",
      "Epoch 1538/2001 completed. Loss: 0.0373\n",
      "Epoch 1539/2001 completed. Loss: 0.0390\n",
      "Epoch 1540/2001 completed. Loss: 0.0390\n",
      "Epoch 1541/2001 completed. Loss: 0.0408\n",
      "Epoch 1542/2001 completed. Loss: 0.0393\n",
      "Epoch 1543/2001 completed. Loss: 0.0376\n",
      "Epoch 1544/2001 completed. Loss: 0.0396\n",
      "Epoch 1545/2001 completed. Loss: 0.0381\n",
      "Epoch 1546/2001 completed. Loss: 0.0390\n",
      "Epoch 1547/2001 completed. Loss: 0.0384\n",
      "Epoch 1548/2001 completed. Loss: 0.0387\n",
      "Epoch 1549/2001 completed. Loss: 0.0378\n",
      "Epoch 1550/2001 completed. Loss: 0.0384\n",
      "Epoch 1551/2001 completed. Loss: 0.0363\n",
      "Epoch 1552/2001 completed. Loss: 0.0387\n",
      "Epoch 1553/2001 completed. Loss: 0.0374\n",
      "Epoch 1554/2001 completed. Loss: 0.0395\n",
      "Epoch 1555/2001 completed. Loss: 0.0377\n",
      "Epoch 1556/2001 completed. Loss: 0.0389\n",
      "Epoch 1557/2001 completed. Loss: 0.0403\n",
      "Epoch 1558/2001 completed. Loss: 0.0394\n",
      "Epoch 1559/2001 completed. Loss: 0.0382\n",
      "Epoch 1560/2001 completed. Loss: 0.0402\n",
      "Epoch 1561/2001 completed. Loss: 0.0395\n",
      "Epoch 1562/2001 completed. Loss: 0.0385\n",
      "Epoch 1563/2001 completed. Loss: 0.0390\n",
      "Epoch 1564/2001 completed. Loss: 0.0367\n",
      "Epoch 1565/2001 completed. Loss: 0.0387\n",
      "Epoch 1566/2001 completed. Loss: 0.0401\n",
      "Epoch 1567/2001 completed. Loss: 0.0375\n",
      "Epoch 1568/2001 completed. Loss: 0.0383\n",
      "Epoch 1569/2001 completed. Loss: 0.0391\n",
      "Epoch 1570/2001 completed. Loss: 0.0377\n",
      "Epoch 1571/2001 completed. Loss: 0.0388\n",
      "Epoch 1572/2001 completed. Loss: 0.0407\n",
      "Epoch 1573/2001 completed. Loss: 0.0392\n",
      "Epoch 1574/2001 completed. Loss: 0.0387\n",
      "Epoch 1575/2001 completed. Loss: 0.0393\n",
      "Epoch 1576/2001 completed. Loss: 0.0391\n",
      "Epoch 1577/2001 completed. Loss: 0.0384\n",
      "Epoch 1578/2001 completed. Loss: 0.0377\n",
      "Epoch 1579/2001 completed. Loss: 0.0379\n",
      "Epoch 1580/2001 completed. Loss: 0.0396\n",
      "Epoch 1581/2001 completed. Loss: 0.0386\n",
      "Epoch 1582/2001 completed. Loss: 0.0393\n",
      "Epoch 1583/2001 completed. Loss: 0.0388\n",
      "Epoch 1584/2001 completed. Loss: 0.0402\n",
      "Epoch 1585/2001 completed. Loss: 0.0378\n",
      "Epoch 1586/2001 completed. Loss: 0.0376\n",
      "Epoch 1587/2001 completed. Loss: 0.0374\n",
      "Epoch 1588/2001 completed. Loss: 0.0383\n",
      "Epoch 1589/2001 completed. Loss: 0.0369\n",
      "Epoch 1590/2001 completed. Loss: 0.0372\n",
      "Epoch 1591/2001 completed. Loss: 0.0367\n",
      "Epoch 1592/2001 completed. Loss: 0.0399\n",
      "Epoch 1593/2001 completed. Loss: 0.0402\n",
      "Epoch 1594/2001 completed. Loss: 0.0408\n",
      "Epoch 1595/2001 completed. Loss: 0.0375\n",
      "Epoch 1596/2001 completed. Loss: 0.0387\n",
      "Epoch 1597/2001 completed. Loss: 0.0387\n",
      "Epoch 1598/2001 completed. Loss: 0.0376\n",
      "Epoch 1599/2001 completed. Loss: 0.0380\n",
      "Epoch 1600/2001 completed. Loss: 0.0379\n",
      "Epoch 1601/2001 completed. Loss: 0.0382\n",
      "Epoch 1602/2001 completed. Loss: 0.0362\n",
      "Epoch 1603/2001 completed. Loss: 0.0379\n",
      "Epoch 1604/2001 completed. Loss: 0.0397\n",
      "Epoch 1605/2001 completed. Loss: 0.0383\n",
      "Epoch 1606/2001 completed. Loss: 0.0380\n",
      "Epoch 1607/2001 completed. Loss: 0.0403\n",
      "Epoch 1608/2001 completed. Loss: 0.0396\n",
      "Epoch 1609/2001 completed. Loss: 0.0386\n",
      "Epoch 1610/2001 completed. Loss: 0.0361\n",
      "Epoch 1611/2001 completed. Loss: 0.0374\n",
      "Epoch 1612/2001 completed. Loss: 0.0403\n",
      "Epoch 1613/2001 completed. Loss: 0.0366\n",
      "Epoch 1614/2001 completed. Loss: 0.0392\n",
      "Epoch 1615/2001 completed. Loss: 0.0382\n",
      "Epoch 1616/2001 completed. Loss: 0.0383\n",
      "Epoch 1617/2001 completed. Loss: 0.0396\n",
      "Epoch 1618/2001 completed. Loss: 0.0369\n",
      "Epoch 1619/2001 completed. Loss: 0.0372\n",
      "Epoch 1620/2001 completed. Loss: 0.0389\n",
      "Epoch 1621/2001 completed. Loss: 0.0390\n",
      "Epoch 1622/2001 completed. Loss: 0.0373\n",
      "Epoch 1623/2001 completed. Loss: 0.0393\n",
      "Epoch 1624/2001 completed. Loss: 0.0387\n",
      "Epoch 1625/2001 completed. Loss: 0.0392\n",
      "Epoch 1626/2001 completed. Loss: 0.0374\n",
      "Epoch 1627/2001 completed. Loss: 0.0374\n",
      "Epoch 1628/2001 completed. Loss: 0.0398\n",
      "Epoch 1629/2001 completed. Loss: 0.0374\n",
      "Epoch 1630/2001 completed. Loss: 0.0369\n",
      "Epoch 1631/2001 completed. Loss: 0.0382\n",
      "Epoch 1632/2001 completed. Loss: 0.0402\n",
      "Epoch 1633/2001 completed. Loss: 0.0378\n",
      "Epoch 1634/2001 completed. Loss: 0.0393\n",
      "Epoch 1635/2001 completed. Loss: 0.0383\n",
      "Epoch 1636/2001 completed. Loss: 0.0368\n",
      "Epoch 1637/2001 completed. Loss: 0.0370\n",
      "Epoch 1638/2001 completed. Loss: 0.0374\n",
      "Epoch 1639/2001 completed. Loss: 0.0384\n",
      "Epoch 1640/2001 completed. Loss: 0.0379\n",
      "Epoch 1641/2001 completed. Loss: 0.0383\n",
      "Epoch 1642/2001 completed. Loss: 0.0386\n",
      "Epoch 1643/2001 completed. Loss: 0.0364\n",
      "Epoch 1644/2001 completed. Loss: 0.0365\n",
      "Epoch 1645/2001 completed. Loss: 0.0377\n",
      "Epoch 1646/2001 completed. Loss: 0.0374\n",
      "Epoch 1647/2001 completed. Loss: 0.0386\n",
      "Epoch 1648/2001 completed. Loss: 0.0390\n",
      "Epoch 1649/2001 completed. Loss: 0.0374\n",
      "Epoch 1650/2001 completed. Loss: 0.0369\n",
      "Epoch 1651/2001 completed. Loss: 0.0382\n",
      "Epoch 1652/2001 completed. Loss: 0.0383\n",
      "Epoch 1653/2001 completed. Loss: 0.0389\n",
      "Epoch 1654/2001 completed. Loss: 0.0360\n",
      "Epoch 1655/2001 completed. Loss: 0.0400\n",
      "Epoch 1656/2001 completed. Loss: 0.0384\n",
      "Epoch 1657/2001 completed. Loss: 0.0381\n",
      "Epoch 1658/2001 completed. Loss: 0.0395\n",
      "Epoch 1659/2001 completed. Loss: 0.0379\n",
      "Epoch 1660/2001 completed. Loss: 0.0367\n",
      "Epoch 1661/2001 completed. Loss: 0.0390\n",
      "Epoch 1662/2001 completed. Loss: 0.0370\n",
      "Epoch 1663/2001 completed. Loss: 0.0356\n",
      "Epoch 1664/2001 completed. Loss: 0.0380\n",
      "Epoch 1665/2001 completed. Loss: 0.0368\n",
      "Epoch 1666/2001 completed. Loss: 0.0386\n",
      "Epoch 1667/2001 completed. Loss: 0.0365\n",
      "Epoch 1668/2001 completed. Loss: 0.0380\n",
      "Epoch 1669/2001 completed. Loss: 0.0378\n",
      "Epoch 1670/2001 completed. Loss: 0.0370\n",
      "Epoch 1671/2001 completed. Loss: 0.0382\n",
      "Epoch 1672/2001 completed. Loss: 0.0388\n",
      "Epoch 1673/2001 completed. Loss: 0.0383\n",
      "Epoch 1674/2001 completed. Loss: 0.0392\n",
      "Epoch 1675/2001 completed. Loss: 0.0395\n",
      "Epoch 1676/2001 completed. Loss: 0.0369\n",
      "Epoch 1677/2001 completed. Loss: 0.0395\n",
      "Epoch 1678/2001 completed. Loss: 0.0397\n",
      "Epoch 1679/2001 completed. Loss: 0.0381\n",
      "Epoch 1680/2001 completed. Loss: 0.0385\n",
      "Epoch 1681/2001 completed. Loss: 0.0357\n",
      "Epoch 1682/2001 completed. Loss: 0.0379\n",
      "Epoch 1683/2001 completed. Loss: 0.0383\n",
      "Epoch 1684/2001 completed. Loss: 0.0377\n",
      "Epoch 1685/2001 completed. Loss: 0.0383\n",
      "Epoch 1686/2001 completed. Loss: 0.0389\n",
      "Epoch 1687/2001 completed. Loss: 0.0380\n",
      "Epoch 1688/2001 completed. Loss: 0.0382\n",
      "Epoch 1689/2001 completed. Loss: 0.0383\n",
      "Epoch 1690/2001 completed. Loss: 0.0370\n",
      "Epoch 1691/2001 completed. Loss: 0.0381\n",
      "Epoch 1692/2001 completed. Loss: 0.0360\n",
      "Epoch 1693/2001 completed. Loss: 0.0381\n",
      "Epoch 1694/2001 completed. Loss: 0.0401\n",
      "Epoch 1695/2001 completed. Loss: 0.0409\n",
      "Epoch 1696/2001 completed. Loss: 0.0366\n",
      "Epoch 1697/2001 completed. Loss: 0.0375\n",
      "Epoch 1698/2001 completed. Loss: 0.0390\n",
      "Epoch 1699/2001 completed. Loss: 0.0394\n",
      "Epoch 1700/2001 completed. Loss: 0.0396\n",
      "Epoch 1701/2001 completed. Loss: 0.0390\n",
      "Epoch 1702/2001 completed. Loss: 0.0367\n",
      "Epoch 1703/2001 completed. Loss: 0.0386\n",
      "Epoch 1704/2001 completed. Loss: 0.0375\n",
      "Epoch 1705/2001 completed. Loss: 0.0378\n",
      "Epoch 1706/2001 completed. Loss: 0.0377\n",
      "Epoch 1707/2001 completed. Loss: 0.0396\n",
      "Epoch 1708/2001 completed. Loss: 0.0376\n",
      "Epoch 1709/2001 completed. Loss: 0.0397\n",
      "Epoch 1710/2001 completed. Loss: 0.0369\n",
      "Epoch 1711/2001 completed. Loss: 0.0361\n",
      "Epoch 1712/2001 completed. Loss: 0.0375\n",
      "Epoch 1713/2001 completed. Loss: 0.0384\n",
      "Epoch 1714/2001 completed. Loss: 0.0365\n",
      "Epoch 1715/2001 completed. Loss: 0.0383\n",
      "Epoch 1716/2001 completed. Loss: 0.0365\n",
      "Epoch 1717/2001 completed. Loss: 0.0360\n",
      "Epoch 1718/2001 completed. Loss: 0.0374\n",
      "Epoch 1719/2001 completed. Loss: 0.0380\n",
      "Epoch 1720/2001 completed. Loss: 0.0366\n",
      "Epoch 1721/2001 completed. Loss: 0.0364\n",
      "Epoch 1722/2001 completed. Loss: 0.0366\n",
      "Epoch 1723/2001 completed. Loss: 0.0353\n",
      "Epoch 1724/2001 completed. Loss: 0.0373\n",
      "Epoch 1725/2001 completed. Loss: 0.0399\n",
      "Epoch 1726/2001 completed. Loss: 0.0385\n",
      "Epoch 1727/2001 completed. Loss: 0.0383\n",
      "Epoch 1728/2001 completed. Loss: 0.0384\n",
      "Epoch 1729/2001 completed. Loss: 0.0360\n",
      "Epoch 1730/2001 completed. Loss: 0.0373\n",
      "Epoch 1731/2001 completed. Loss: 0.0372\n",
      "Epoch 1732/2001 completed. Loss: 0.0392\n",
      "Epoch 1733/2001 completed. Loss: 0.0382\n",
      "Epoch 1734/2001 completed. Loss: 0.0383\n",
      "Epoch 1735/2001 completed. Loss: 0.0391\n",
      "Epoch 1736/2001 completed. Loss: 0.0364\n",
      "Epoch 1737/2001 completed. Loss: 0.0365\n",
      "Epoch 1738/2001 completed. Loss: 0.0380\n",
      "Epoch 1739/2001 completed. Loss: 0.0375\n",
      "Epoch 1740/2001 completed. Loss: 0.0381\n",
      "Epoch 1741/2001 completed. Loss: 0.0377\n",
      "Epoch 1742/2001 completed. Loss: 0.0376\n",
      "Epoch 1743/2001 completed. Loss: 0.0374\n",
      "Epoch 1744/2001 completed. Loss: 0.0355\n",
      "Epoch 1745/2001 completed. Loss: 0.0379\n",
      "Epoch 1746/2001 completed. Loss: 0.0388\n",
      "Epoch 1747/2001 completed. Loss: 0.0379\n",
      "Epoch 1748/2001 completed. Loss: 0.0381\n",
      "Epoch 1749/2001 completed. Loss: 0.0363\n",
      "Epoch 1750/2001 completed. Loss: 0.0380\n",
      "Epoch 1751/2001 completed. Loss: 0.0367\n",
      "Epoch 1752/2001 completed. Loss: 0.0380\n",
      "Epoch 1753/2001 completed. Loss: 0.0381\n",
      "Epoch 1754/2001 completed. Loss: 0.0360\n",
      "Epoch 1755/2001 completed. Loss: 0.0366\n",
      "Epoch 1756/2001 completed. Loss: 0.0385\n",
      "Epoch 1757/2001 completed. Loss: 0.0372\n",
      "Epoch 1758/2001 completed. Loss: 0.0351\n",
      "Epoch 1759/2001 completed. Loss: 0.0379\n",
      "Epoch 1760/2001 completed. Loss: 0.0382\n",
      "Epoch 1761/2001 completed. Loss: 0.0370\n",
      "Epoch 1762/2001 completed. Loss: 0.0376\n",
      "Epoch 1763/2001 completed. Loss: 0.0391\n",
      "Epoch 1764/2001 completed. Loss: 0.0396\n",
      "Epoch 1765/2001 completed. Loss: 0.0391\n",
      "Epoch 1766/2001 completed. Loss: 0.0388\n",
      "Epoch 1767/2001 completed. Loss: 0.0367\n",
      "Epoch 1768/2001 completed. Loss: 0.0364\n",
      "Epoch 1769/2001 completed. Loss: 0.0385\n",
      "Epoch 1770/2001 completed. Loss: 0.0405\n",
      "Epoch 1771/2001 completed. Loss: 0.0355\n",
      "Epoch 1772/2001 completed. Loss: 0.0361\n",
      "Epoch 1773/2001 completed. Loss: 0.0375\n",
      "Epoch 1774/2001 completed. Loss: 0.0374\n",
      "Epoch 1775/2001 completed. Loss: 0.0369\n",
      "Epoch 1776/2001 completed. Loss: 0.0380\n",
      "Epoch 1777/2001 completed. Loss: 0.0388\n",
      "Epoch 1778/2001 completed. Loss: 0.0385\n",
      "Epoch 1779/2001 completed. Loss: 0.0390\n",
      "Epoch 1780/2001 completed. Loss: 0.0384\n",
      "Epoch 1781/2001 completed. Loss: 0.0380\n",
      "Epoch 1782/2001 completed. Loss: 0.0361\n",
      "Epoch 1783/2001 completed. Loss: 0.0372\n",
      "Epoch 1784/2001 completed. Loss: 0.0372\n",
      "Epoch 1785/2001 completed. Loss: 0.0358\n",
      "Epoch 1786/2001 completed. Loss: 0.0387\n",
      "Epoch 1787/2001 completed. Loss: 0.0376\n",
      "Epoch 1788/2001 completed. Loss: 0.0367\n",
      "Epoch 1789/2001 completed. Loss: 0.0366\n",
      "Epoch 1790/2001 completed. Loss: 0.0371\n",
      "Epoch 1791/2001 completed. Loss: 0.0351\n",
      "Epoch 1792/2001 completed. Loss: 0.0380\n",
      "Epoch 1793/2001 completed. Loss: 0.0356\n",
      "Epoch 1794/2001 completed. Loss: 0.0385\n",
      "Epoch 1795/2001 completed. Loss: 0.0375\n",
      "Epoch 1796/2001 completed. Loss: 0.0376\n",
      "Epoch 1797/2001 completed. Loss: 0.0377\n",
      "Epoch 1798/2001 completed. Loss: 0.0379\n",
      "Epoch 1799/2001 completed. Loss: 0.0399\n",
      "Epoch 1800/2001 completed. Loss: 0.0377\n",
      "Epoch 1801/2001 completed. Loss: 0.0374\n",
      "Epoch 1802/2001 completed. Loss: 0.0374\n",
      "Epoch 1803/2001 completed. Loss: 0.0401\n",
      "Epoch 1804/2001 completed. Loss: 0.0379\n",
      "Epoch 1805/2001 completed. Loss: 0.0365\n",
      "Epoch 1806/2001 completed. Loss: 0.0383\n",
      "Epoch 1807/2001 completed. Loss: 0.0377\n",
      "Epoch 1808/2001 completed. Loss: 0.0363\n",
      "Epoch 1809/2001 completed. Loss: 0.0383\n",
      "Epoch 1810/2001 completed. Loss: 0.0377\n",
      "Epoch 1811/2001 completed. Loss: 0.0390\n",
      "Epoch 1812/2001 completed. Loss: 0.0391\n",
      "Epoch 1813/2001 completed. Loss: 0.0379\n",
      "Epoch 1814/2001 completed. Loss: 0.0353\n",
      "Epoch 1815/2001 completed. Loss: 0.0363\n",
      "Epoch 1816/2001 completed. Loss: 0.0380\n",
      "Epoch 1817/2001 completed. Loss: 0.0351\n",
      "Epoch 1818/2001 completed. Loss: 0.0390\n",
      "Epoch 1819/2001 completed. Loss: 0.0360\n",
      "Epoch 1820/2001 completed. Loss: 0.0353\n",
      "Epoch 1821/2001 completed. Loss: 0.0355\n",
      "Epoch 1822/2001 completed. Loss: 0.0371\n",
      "Epoch 1823/2001 completed. Loss: 0.0373\n",
      "Epoch 1824/2001 completed. Loss: 0.0356\n",
      "Epoch 1825/2001 completed. Loss: 0.0380\n",
      "Epoch 1826/2001 completed. Loss: 0.0365\n",
      "Epoch 1827/2001 completed. Loss: 0.0354\n",
      "Epoch 1828/2001 completed. Loss: 0.0382\n",
      "Epoch 1829/2001 completed. Loss: 0.0369\n",
      "Epoch 1830/2001 completed. Loss: 0.0375\n",
      "Epoch 1831/2001 completed. Loss: 0.0398\n",
      "Epoch 1832/2001 completed. Loss: 0.0362\n",
      "Epoch 1833/2001 completed. Loss: 0.0371\n",
      "Epoch 1834/2001 completed. Loss: 0.0367\n",
      "Epoch 1835/2001 completed. Loss: 0.0354\n",
      "Epoch 1836/2001 completed. Loss: 0.0356\n",
      "Epoch 1837/2001 completed. Loss: 0.0374\n",
      "Epoch 1838/2001 completed. Loss: 0.0379\n",
      "Epoch 1839/2001 completed. Loss: 0.0363\n",
      "Epoch 1840/2001 completed. Loss: 0.0385\n",
      "Epoch 1841/2001 completed. Loss: 0.0372\n",
      "Epoch 1842/2001 completed. Loss: 0.0373\n",
      "Epoch 1843/2001 completed. Loss: 0.0395\n",
      "Epoch 1844/2001 completed. Loss: 0.0383\n",
      "Epoch 1845/2001 completed. Loss: 0.0390\n",
      "Epoch 1846/2001 completed. Loss: 0.0380\n",
      "Epoch 1847/2001 completed. Loss: 0.0382\n",
      "Epoch 1848/2001 completed. Loss: 0.0369\n",
      "Epoch 1849/2001 completed. Loss: 0.0385\n",
      "Epoch 1850/2001 completed. Loss: 0.0354\n",
      "Epoch 1851/2001 completed. Loss: 0.0369\n",
      "Epoch 1852/2001 completed. Loss: 0.0379\n",
      "Epoch 1853/2001 completed. Loss: 0.0377\n",
      "Epoch 1854/2001 completed. Loss: 0.0354\n",
      "Epoch 1855/2001 completed. Loss: 0.0392\n",
      "Epoch 1856/2001 completed. Loss: 0.0373\n",
      "Epoch 1857/2001 completed. Loss: 0.0381\n",
      "Epoch 1858/2001 completed. Loss: 0.0365\n",
      "Epoch 1859/2001 completed. Loss: 0.0366\n",
      "Epoch 1860/2001 completed. Loss: 0.0385\n",
      "Epoch 1861/2001 completed. Loss: 0.0364\n",
      "Epoch 1862/2001 completed. Loss: 0.0354\n",
      "Epoch 1863/2001 completed. Loss: 0.0405\n",
      "Epoch 1864/2001 completed. Loss: 0.0368\n",
      "Epoch 1865/2001 completed. Loss: 0.0364\n",
      "Epoch 1866/2001 completed. Loss: 0.0388\n",
      "Epoch 1867/2001 completed. Loss: 0.0366\n",
      "Epoch 1868/2001 completed. Loss: 0.0361\n",
      "Epoch 1869/2001 completed. Loss: 0.0387\n",
      "Epoch 1870/2001 completed. Loss: 0.0405\n",
      "Epoch 1871/2001 completed. Loss: 0.0365\n",
      "Epoch 1872/2001 completed. Loss: 0.0368\n",
      "Epoch 1873/2001 completed. Loss: 0.0367\n",
      "Epoch 1874/2001 completed. Loss: 0.0371\n",
      "Epoch 1875/2001 completed. Loss: 0.0369\n",
      "Epoch 1876/2001 completed. Loss: 0.0359\n",
      "Epoch 1877/2001 completed. Loss: 0.0347\n",
      "Epoch 1878/2001 completed. Loss: 0.0360\n",
      "Epoch 1879/2001 completed. Loss: 0.0345\n",
      "Epoch 1880/2001 completed. Loss: 0.0369\n",
      "Epoch 1881/2001 completed. Loss: 0.0365\n",
      "Epoch 1882/2001 completed. Loss: 0.0382\n",
      "Epoch 1883/2001 completed. Loss: 0.0376\n",
      "Epoch 1884/2001 completed. Loss: 0.0359\n",
      "Epoch 1885/2001 completed. Loss: 0.0388\n",
      "Epoch 1886/2001 completed. Loss: 0.0392\n",
      "Epoch 1887/2001 completed. Loss: 0.0379\n",
      "Epoch 1888/2001 completed. Loss: 0.0350\n",
      "Epoch 1889/2001 completed. Loss: 0.0378\n",
      "Epoch 1890/2001 completed. Loss: 0.0375\n",
      "Epoch 1891/2001 completed. Loss: 0.0375\n",
      "Epoch 1892/2001 completed. Loss: 0.0375\n",
      "Epoch 1893/2001 completed. Loss: 0.0359\n",
      "Epoch 1894/2001 completed. Loss: 0.0364\n",
      "Epoch 1895/2001 completed. Loss: 0.0368\n",
      "Epoch 1896/2001 completed. Loss: 0.0370\n",
      "Epoch 1897/2001 completed. Loss: 0.0382\n",
      "Epoch 1898/2001 completed. Loss: 0.0386\n",
      "Epoch 1899/2001 completed. Loss: 0.0354\n",
      "Epoch 1900/2001 completed. Loss: 0.0360\n",
      "Epoch 1901/2001 completed. Loss: 0.0390\n",
      "Epoch 1902/2001 completed. Loss: 0.0369\n",
      "Epoch 1903/2001 completed. Loss: 0.0378\n",
      "Epoch 1904/2001 completed. Loss: 0.0368\n",
      "Epoch 1905/2001 completed. Loss: 0.0358\n",
      "Epoch 1906/2001 completed. Loss: 0.0363\n",
      "Epoch 1907/2001 completed. Loss: 0.0393\n",
      "Epoch 1908/2001 completed. Loss: 0.0371\n",
      "Epoch 1909/2001 completed. Loss: 0.0365\n",
      "Epoch 1910/2001 completed. Loss: 0.0371\n",
      "Epoch 1911/2001 completed. Loss: 0.0366\n",
      "Epoch 1912/2001 completed. Loss: 0.0377\n",
      "Epoch 1913/2001 completed. Loss: 0.0370\n",
      "Epoch 1914/2001 completed. Loss: 0.0367\n",
      "Epoch 1915/2001 completed. Loss: 0.0370\n",
      "Epoch 1916/2001 completed. Loss: 0.0373\n",
      "Epoch 1917/2001 completed. Loss: 0.0380\n",
      "Epoch 1918/2001 completed. Loss: 0.0364\n",
      "Epoch 1919/2001 completed. Loss: 0.0378\n",
      "Epoch 1920/2001 completed. Loss: 0.0410\n",
      "Epoch 1921/2001 completed. Loss: 0.0366\n",
      "Epoch 1922/2001 completed. Loss: 0.0369\n",
      "Epoch 1923/2001 completed. Loss: 0.0378\n",
      "Epoch 1924/2001 completed. Loss: 0.0361\n",
      "Epoch 1925/2001 completed. Loss: 0.0365\n",
      "Epoch 1926/2001 completed. Loss: 0.0363\n",
      "Epoch 1927/2001 completed. Loss: 0.0377\n",
      "Epoch 1928/2001 completed. Loss: 0.0367\n",
      "Epoch 1929/2001 completed. Loss: 0.0366\n",
      "Epoch 1930/2001 completed. Loss: 0.0374\n",
      "Epoch 1931/2001 completed. Loss: 0.0367\n",
      "Epoch 1932/2001 completed. Loss: 0.0368\n",
      "Epoch 1933/2001 completed. Loss: 0.0358\n",
      "Epoch 1934/2001 completed. Loss: 0.0372\n",
      "Epoch 1935/2001 completed. Loss: 0.0359\n",
      "Epoch 1936/2001 completed. Loss: 0.0372\n",
      "Epoch 1937/2001 completed. Loss: 0.0374\n",
      "Epoch 1938/2001 completed. Loss: 0.0366\n",
      "Epoch 1939/2001 completed. Loss: 0.0387\n",
      "Epoch 1940/2001 completed. Loss: 0.0353\n",
      "Epoch 1941/2001 completed. Loss: 0.0369\n",
      "Epoch 1942/2001 completed. Loss: 0.0378\n",
      "Epoch 1943/2001 completed. Loss: 0.0383\n",
      "Epoch 1944/2001 completed. Loss: 0.0346\n",
      "Epoch 1945/2001 completed. Loss: 0.0370\n",
      "Epoch 1946/2001 completed. Loss: 0.0357\n",
      "Epoch 1947/2001 completed. Loss: 0.0373\n",
      "Epoch 1948/2001 completed. Loss: 0.0365\n",
      "Epoch 1949/2001 completed. Loss: 0.0376\n",
      "Epoch 1950/2001 completed. Loss: 0.0359\n",
      "Epoch 1951/2001 completed. Loss: 0.0372\n",
      "Epoch 1952/2001 completed. Loss: 0.0349\n",
      "Epoch 1953/2001 completed. Loss: 0.0388\n",
      "Epoch 1954/2001 completed. Loss: 0.0349\n",
      "Epoch 1955/2001 completed. Loss: 0.0370\n",
      "Epoch 1956/2001 completed. Loss: 0.0375\n",
      "Epoch 1957/2001 completed. Loss: 0.0364\n",
      "Epoch 1958/2001 completed. Loss: 0.0371\n",
      "Epoch 1959/2001 completed. Loss: 0.0383\n",
      "Epoch 1960/2001 completed. Loss: 0.0366\n",
      "Epoch 1961/2001 completed. Loss: 0.0362\n",
      "Epoch 1962/2001 completed. Loss: 0.0362\n",
      "Epoch 1963/2001 completed. Loss: 0.0372\n",
      "Epoch 1964/2001 completed. Loss: 0.0361\n",
      "Epoch 1965/2001 completed. Loss: 0.0362\n",
      "Epoch 1966/2001 completed. Loss: 0.0374\n",
      "Epoch 1967/2001 completed. Loss: 0.0336\n",
      "Epoch 1968/2001 completed. Loss: 0.0352\n",
      "Epoch 1969/2001 completed. Loss: 0.0347\n",
      "Epoch 1970/2001 completed. Loss: 0.0383\n",
      "Epoch 1971/2001 completed. Loss: 0.0375\n",
      "Epoch 1972/2001 completed. Loss: 0.0354\n",
      "Epoch 1973/2001 completed. Loss: 0.0374\n",
      "Epoch 1974/2001 completed. Loss: 0.0363\n",
      "Epoch 1975/2001 completed. Loss: 0.0391\n",
      "Epoch 1976/2001 completed. Loss: 0.0375\n",
      "Epoch 1977/2001 completed. Loss: 0.0369\n",
      "Epoch 1978/2001 completed. Loss: 0.0353\n",
      "Epoch 1979/2001 completed. Loss: 0.0375\n",
      "Epoch 1980/2001 completed. Loss: 0.0370\n",
      "Epoch 1981/2001 completed. Loss: 0.0363\n",
      "Epoch 1982/2001 completed. Loss: 0.0353\n",
      "Epoch 1983/2001 completed. Loss: 0.0364\n",
      "Epoch 1984/2001 completed. Loss: 0.0360\n",
      "Epoch 1985/2001 completed. Loss: 0.0386\n",
      "Epoch 1986/2001 completed. Loss: 0.0365\n",
      "Epoch 1987/2001 completed. Loss: 0.0359\n",
      "Epoch 1988/2001 completed. Loss: 0.0376\n",
      "Epoch 1989/2001 completed. Loss: 0.0373\n",
      "Epoch 1990/2001 completed. Loss: 0.0366\n",
      "Epoch 1991/2001 completed. Loss: 0.0353\n",
      "Epoch 1992/2001 completed. Loss: 0.0357\n",
      "Epoch 1993/2001 completed. Loss: 0.0397\n",
      "Epoch 1994/2001 completed. Loss: 0.0376\n",
      "Epoch 1995/2001 completed. Loss: 0.0382\n",
      "Epoch 1996/2001 completed. Loss: 0.0380\n",
      "Epoch 1997/2001 completed. Loss: 0.0364\n",
      "Epoch 1998/2001 completed. Loss: 0.0343\n",
      "Epoch 1999/2001 completed. Loss: 0.0353\n",
      "Epoch 2000/2001 completed. Loss: 0.0359\n",
      "Epoch 2001/2001 completed. Loss: 0.0370\n",
      "Model weights saved to /home/22058122/BioClinicalBERT/fine_tune/fine_tuned_clinical_bert_sequentialclassifier_{epoch}.pt\n",
      "Training completed in 142634.90 seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing completed in 263.97 seconds.\n",
      "Accuracy: 0.9883\n",
      "Misclassification Error Rate: 0.0117\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99     18074\n",
      "     Class 1       0.99      0.99      0.99     18214\n",
      "\n",
      "    accuracy                           0.99     36288\n",
      "   macro avg       0.99      0.99      0.99     36288\n",
      "weighted avg       0.99      0.99      0.99     36288\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if CUDA is available and move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Split the data into 20% for training and 80% for testing\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(inputs['input_ids'], labels, test_size=0.8, random_state=13)\n",
    "trainMasks, testMasks = train_test_split(inputs['attention_mask'], test_size=0.8, random_state=13)\n",
    "\n",
    "# Create DataLoader for the training and testing datasets\n",
    "trainData = TensorDataset(xTrain, trainMasks, yTrain)\n",
    "testData = TensorDataset(xTest, testMasks, yTest)\n",
    "trainDataLoader = DataLoader(trainData, batch_size=32)\n",
    "testDataLoader = DataLoader(testData, batch_size=32)\n",
    "\n",
    "# Fine-tuning settings\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 2001\n",
    "loss_values = []\n",
    "\n",
    "# Measure training time\n",
    "start_training_time = time.time()\n",
    "\n",
    "# Fine-tuning loop (on the 20% training set)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in trainDataLoader:\n",
    "        bInputIds, bInputMask, bLabels = [b.to(device) for b in batch]  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(bInputIds, attention_mask=bInputMask, labels=bLabels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss for this epoch\n",
    "    avg_epoch_loss = epoch_loss / len(trainDataLoader)\n",
    "    loss_values.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed. Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model weights after training\n",
    "\n",
    "# Use os.path.expanduser to expand the '~' to the full path\n",
    "model_save_path = os.path.expanduser(\"~/BioClinicalBERT/fine_tune/fine_tuned_clinical_bert_sequentialclassifier_{epoch}.pt\")\n",
    "# Save the model at the specified path\n",
    "torch.save(model.state_dict(), model_save_path.format(epoch=epoch))\n",
    "# model_save_path = \"~/BioClinicalBERT/fine_tune/fine_tuned_clinical_bert_ipynb.pt\"\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")\n",
    "\n",
    "end_training_time = time.time()\n",
    "training_time = end_training_time - start_training_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs + 1), loss_values, marker='o', label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Measure testing time\n",
    "start_testing_time = time.time()\n",
    "\n",
    "# Set model to evaluation mode for testing\n",
    "model.eval()\n",
    "\n",
    "# Function to predict labels using the model\n",
    "def predictLabels(dataLoader):\n",
    "    predictions = []\n",
    "    for batch in dataLoader:\n",
    "        bInputIds, bInputMask, _ = [b.to(device) for b in batch]  # Move data to GPU\n",
    "        with torch.no_grad():\n",
    "            outputs = model(bInputIds, attention_mask=bInputMask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()  # Move back to CPU for evaluation\n",
    "            predictions.extend(preds)\n",
    "    return predictions\n",
    "\n",
    "# Predict labels for the test set (80% data)\n",
    "yPred = predictLabels(testDataLoader)\n",
    "\n",
    "end_testing_time = time.time()\n",
    "testing_time = end_testing_time - start_testing_time\n",
    "print(f\"Testing completed in {testing_time:.2f} seconds.\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(yTest, yPred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate misclassification error rate\n",
    "misclassificationErrorRate = 1 - accuracy\n",
    "print(f\"Misclassification Error Rate: {misclassificationErrorRate:.4f}\")\n",
    "\n",
    "# Generate a classification report (precision, recall, F1-score)\n",
    "report = classification_report(yTest, yPred, target_names=['Class 0', 'Class 1'])\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "confMatrix = confusion_matrix(yTest, yPred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confMatrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaecf4b8-0f41-4d51-9f21-daba8de80168",
   "metadata": {},
   "source": [
    "# The preliminary model exhibited poor performance during testing, achieving a classification accuracy of below 40% on the gold standard dataset. This result is significantly below the benchmark of 92% set for evaluating the models effectiveness. \n",
    "\n",
    "# Further finetuning is required. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioClinicalBERT",
   "language": "python",
   "name": "bioclinicalbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
